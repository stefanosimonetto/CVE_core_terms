{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on core terms and contextual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 1.8751 - a\n",
      "Epoch 1 - F1 Score: 0.5801\n",
      "Saved best model\n",
      "[0.5800555209446794]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.8730 - accuracy: 0.5078 - val_loss: 1.4126 - val_accuracy: 0.6088\n",
      "Epoch 2/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.3322 - accu\n",
      "Epoch 2 - F1 Score: 0.6167\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.3325 - accuracy: 0.6258 - val_loss: 1.3009 - val_accuracy: 0.6340\n",
      "Epoch 3/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.2252 - accu\n",
      "Epoch 3 - F1 Score: 0.6413\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.2253 - accuracy: 0.6554 - val_loss: 1.2110 - val_accuracy: 0.6558\n",
      "Epoch 4/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1674 - accura\n",
      "Epoch 4 - F1 Score: 0.6510\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.1676 - accuracy: 0.6693 - val_loss: 1.1981 - val_accuracy: 0.6640\n",
      "Epoch 5/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1288 - \n",
      "Epoch 5 - F1 Score: 0.6610\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743, 0.6610493144404357]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.1288 - accuracy: 0.6776 - val_loss: 1.1465 - val_accuracy: 0.6748\n",
      "Epoch 6/40\n",
      "260/260 [==============================] - 1s 3ms/step loss:\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0973 - accuracy: 0.6850 - val_loss: 1.1571 - val_accuracy: 0.6723\n",
      "Epoch 7/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.\n",
      "Epoch 7 - F1 Score: 0.6702\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743, 0.6610493144404357, 0.6602948156271925, 0.6702245255695949]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.0714 - accuracy: 0.6922 - val_loss: 1.1317 - val_accuracy: 0.6811\n",
      "Epoch 8/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.0509 \n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0502 - accuracy: 0.6963 - val_loss: 1.1319 - val_accuracy: 0.6817\n",
      "Epoch 9/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.0307 - accura\n",
      "Epoch 9 - F1 Score: 0.6745\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743, 0.6610493144404357, 0.6602948156271925, 0.6702245255695949, 0.6690151746762277, 0.6744720797858907]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.0307 - accuracy: 0.7017 - val_loss: 1.1182 - val_accuracy: 0.6869\n",
      "Epoch 10/40\n",
      "260/260 [==============================] - 1s 3ms/step loss: 1\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.0117 - accuracy: 0.7046 - val_loss: 1.1402 - val_accuracy: 0.6811\n",
      "Epoch 11/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9947 - accu\n",
      "Epoch 11 - F1 Score: 0.6814\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743, 0.6610493144404357, 0.6602948156271925, 0.6702245255695949, 0.6690151746762277, 0.6744720797858907, 0.671651237001122, 0.68138416654634]\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 0.9947 - accuracy: 0.7097 - val_loss: 1.0992 - val_accuracy: 0.6881\n",
      "Epoch 12/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9786 - accuracy: 0.7134 - val_loss: 1.1240 - val_accuracy: 0.6848\n",
      "Epoch 13/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9661 \n",
      "Epoch 13 - F1 Score: 0.6876\n",
      "Saved best model\n",
      "[0.5800555209446794, 0.6167006290291936, 0.6413260315279218, 0.6510021617735743, 0.6610493144404357, 0.6602948156271925, 0.6702245255695949, 0.6690151746762277, 0.6744720797858907, 0.671651237001122, 0.68138416654634, 0.6754964362513385, 0.6875820835450103]\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 0.9664 - accuracy: 0.7166 - val_loss: 1.0915 - val_accuracy: 0.6949\n",
      "Epoch 14/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9502 \n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9502 - accuracy: 0.7207 - val_loss: 1.0985 - val_accuracy: 0.6907\n",
      "Epoch 15/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9367 - accu\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.9372 - accuracy: 0.7225 - val_loss: 1.0987 - val_accuracy: 0.6934\n",
      "Epoch 16/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9216 - accura\n",
      "2336/2336 [==============================] - 6s 2ms/step - loss: 0.9218 - accuracy: 0.7273 - val_loss: 1.1009 - val_accuracy: 0.6931\n",
      "Epoch 17/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9097 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.9097 - accuracy: 0.7309 - val_loss: 1.1179 - val_accuracy: 0.6865\n",
      "Epoch 18/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8975 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.8977 - accuracy: 0.7329 - val_loss: 1.1151 - val_accuracy: 0.6894\n",
      "Epoch 19/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8852 - ac\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.8854 - accuracy: 0.7357 - val_loss: 1.1374 - val_accuracy: 0.6878\n",
      "Epoch 20/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8731 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.8732 - accuracy: 0.7384 - val_loss: 1.1344 - val_accuracy: 0.6873\n",
      "Epoch 21/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8621 - \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.8623 - accuracy: 0.7415 - val_loss: 1.1198 - val_accuracy: 0.6888\n",
      "Epoch 22/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8508 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.8508 - accuracy: 0.7456 - val_loss: 1.1375 - val_accuracy: 0.6847\n",
      "Epoch 23/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8402 - accu\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.8402 - accuracy: 0.7471 - val_loss: 1.1221 - val_accuracy: 0.6861\n",
      "Epoch 24/40\n",
      "260/260 [==============================] - 0s 960us/steposs: 0.8274 - accura\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.8273 - accuracy: 0.7508 - val_loss: 1.1461 - val_accuracy: 0.6831\n",
      "Epoch 25/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8187 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.8186 - accuracy: 0.7514 - val_loss: 1.1636 - val_accuracy: 0.6863\n",
      "Epoch 26/40\n",
      "260/260 [==============================] - 0s 992us/steposs: 0.8088 - accu\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.8092 - accuracy: 0.7546 - val_loss: 1.1897 - val_accuracy: 0.6790\n",
      "Epoch 27/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7983 - accu\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.7981 - accuracy: 0.7568 - val_loss: 1.1413 - val_accuracy: 0.6877\n",
      "Epoch 28/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7891 - ac\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.7889 - accuracy: 0.7595 - val_loss: 1.1629 - val_accuracy: 0.6925\n",
      "Epoch 29/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7778 - ac\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.7778 - accuracy: 0.7635 - val_loss: 1.1790 - val_accuracy: 0.6859\n",
      "Epoch 30/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7683 - \n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.7684 - accuracy: 0.7653 - val_loss: 1.1709 - val_accuracy: 0.6873\n",
      "Epoch 31/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.7598 \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.7601 - accuracy: 0.7664 - val_loss: 1.1831 - val_accuracy: 0.6873\n",
      "Epoch 32/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.750\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.7502 - accuracy: 0.7710 - val_loss: 1.1829 - val_accuracy: 0.6858\n",
      "Epoch 33/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.7393 - accuracy: 0.7737 - val_loss: 1.1981 - val_accuracy: 0.6893\n",
      "Epoch 34/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 0.7320 - accuracy: 0.7733 - val_loss: 1.1987 - val_accuracy: 0.6896\n",
      "Epoch 35/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 0.7235 - accuracy: 0.7771 - val_loss: 1.2054 - val_accuracy: 0.6860\n",
      "Epoch 36/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.713\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7136 - accuracy: 0.7798 - val_loss: 1.2174 - val_accuracy: 0.6814\n",
      "Epoch 37/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.706\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.7066 - accuracy: 0.7802 - val_loss: 1.2170 - val_accuracy: 0.6832\n",
      "Epoch 38/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.6996 - accu\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.6997 - accuracy: 0.7831 - val_loss: 1.2608 - val_accuracy: 0.6782\n",
      "Epoch 39/40\n",
      "260/260 [==============================] - 1s 3ms/step los\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6906 - accuracy: 0.7849 - val_loss: 1.2379 - val_accuracy: 0.6795\n",
      "Epoch 40/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.6831 - ac\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.6831 - accuracy: 0.7858 - val_loss: 1.2487 - val_accuracy: 0.6822\n",
      "444/444 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8284    0.4421    0.5765      1070\n",
      "         120     0.4010    0.8163    0.5378       196\n",
      "         125     0.7394    0.8534    0.7923       532\n",
      "         134     0.4595    0.8947    0.6071        19\n",
      "         189     0.5412    0.7731    0.6367       119\n",
      "         190     0.7000    0.7350    0.7171       200\n",
      "          20     0.4670    0.2012    0.2813       810\n",
      "         200     0.5707    0.5678    0.5692       590\n",
      "         203     0.6000    0.6667    0.6316        27\n",
      "          22     0.8872    0.8803    0.8837       518\n",
      "         254     0.0877    0.1471    0.1099        34\n",
      "         255     0.3368    0.4776    0.3951        67\n",
      "         264     0.5111    0.5050    0.5080       503\n",
      "         269     0.3194    0.4340    0.3680       106\n",
      "         276     0.3077    0.2500    0.2759        64\n",
      "         284     0.2990    0.2358    0.2636       123\n",
      "         287     0.5000    0.6281    0.5568       285\n",
      "         295     0.5238    0.5432    0.5333        81\n",
      "         306     0.4595    0.3617    0.4048        94\n",
      "         310     0.7256    0.8072    0.7643       249\n",
      "         312     0.3448    0.2381    0.2817        42\n",
      "         319     0.5094    0.5294    0.5192        51\n",
      "         326     0.3333    0.1935    0.2449        31\n",
      "         327     0.3143    0.3143    0.3143        35\n",
      "         345     0.4400    0.4231    0.4314        26\n",
      "         347     0.4615    0.5000    0.4800        24\n",
      "         352     0.7343    0.9581    0.8314       453\n",
      "         362     0.7231    0.7705    0.7460       122\n",
      "         399     0.3312    0.5802    0.4216       262\n",
      "         400     0.3036    0.4928    0.3757       138\n",
      "         401     0.5111    0.4423    0.4742        52\n",
      "         415     0.8085    0.9048    0.8539        42\n",
      "         416     0.7869    0.8523    0.8183       325\n",
      "         426     0.6410    0.5952    0.6173        42\n",
      "         427     0.5738    0.7955    0.6667        44\n",
      "         434     0.5724    0.8557    0.6860       194\n",
      "         476     0.6309    0.8430    0.7217       223\n",
      "         502     0.6014    0.8318    0.6980       107\n",
      "         522     0.3907    0.6556    0.4896        90\n",
      "         532     0.4507    0.7273    0.5565        44\n",
      "          59     0.6299    0.7339    0.6780       109\n",
      "         601     0.5472    0.8169    0.6554        71\n",
      "         611     0.7890    0.9348    0.8557        92\n",
      "         617     0.5600    0.7368    0.6364        38\n",
      "         639     0.3061    0.5357    0.3896        28\n",
      "         668     0.1167    0.1556    0.1333        45\n",
      "         732     0.2115    0.4490    0.2876        98\n",
      "          74     0.1322    0.3239    0.1878        71\n",
      "         755     0.2083    0.5357    0.3000        28\n",
      "          77     0.4600    0.4662    0.4631       148\n",
      "         770     0.3000    0.3051    0.3025        59\n",
      "         772     0.4043    0.5588    0.4691        34\n",
      "          78     0.5090    0.7669    0.6119       296\n",
      "         787     0.7471    0.6573    0.6993       890\n",
      "          79     0.9940    0.7332    0.8439      2256\n",
      "         798     0.8286    0.6744    0.7436       129\n",
      "         835     0.5085    0.7317    0.6000        41\n",
      "         843     0.8710    0.7941    0.8308        34\n",
      "         862     0.6389    0.6273    0.6330       220\n",
      "         863     0.4059    0.3475    0.3744       118\n",
      "          89     0.9948    0.7952    0.8839       957\n",
      "         908     0.3784    0.5833    0.4590        24\n",
      "         918     0.5746    0.8556    0.6875        90\n",
      "          94     0.4956    0.5788    0.5340       292\n",
      "\n",
      "    accuracy                         0.6467     14202\n",
      "   macro avg     0.5209    0.6003    0.5453     14202\n",
      "weighted avg     0.6921    0.6467    0.6515     14202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test4.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_core_contextual_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_core_contextual_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_core_contextual_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8284    0.4421    0.5765      1070\n",
      "         120     0.4010    0.8163    0.5378       196\n",
      "         125     0.7394    0.8534    0.7923       532\n",
      "         134     0.4595    0.8947    0.6071        19\n",
      "         189     0.5412    0.7731    0.6367       119\n",
      "         190     0.7000    0.7350    0.7171       200\n",
      "          20     0.4670    0.2012    0.2813       810\n",
      "         200     0.5707    0.5678    0.5692       590\n",
      "         203     0.6000    0.6667    0.6316        27\n",
      "          22     0.8872    0.8803    0.8837       518\n",
      "         254     0.0877    0.1471    0.1099        34\n",
      "         255     0.3368    0.4776    0.3951        67\n",
      "         264     0.5111    0.5050    0.5080       503\n",
      "         269     0.3194    0.4340    0.3680       106\n",
      "         276     0.3077    0.2500    0.2759        64\n",
      "         284     0.2990    0.2358    0.2636       123\n",
      "         287     0.5000    0.6281    0.5568       285\n",
      "         295     0.5238    0.5432    0.5333        81\n",
      "         306     0.4595    0.3617    0.4048        94\n",
      "         310     0.7256    0.8072    0.7643       249\n",
      "         312     0.3448    0.2381    0.2817        42\n",
      "         319     0.5094    0.5294    0.5192        51\n",
      "         326     0.3333    0.1935    0.2449        31\n",
      "         327     0.3143    0.3143    0.3143        35\n",
      "         345     0.4400    0.4231    0.4314        26\n",
      "         347     0.4615    0.5000    0.4800        24\n",
      "         352     0.7343    0.9581    0.8314       453\n",
      "         362     0.7231    0.7705    0.7460       122\n",
      "         399     0.3312    0.5802    0.4216       262\n",
      "         400     0.3036    0.4928    0.3757       138\n",
      "         401     0.5111    0.4423    0.4742        52\n",
      "         415     0.8085    0.9048    0.8539        42\n",
      "         416     0.7869    0.8523    0.8183       325\n",
      "         426     0.6410    0.5952    0.6173        42\n",
      "         427     0.5738    0.7955    0.6667        44\n",
      "         434     0.5724    0.8557    0.6860       194\n",
      "         476     0.6309    0.8430    0.7217       223\n",
      "         502     0.6014    0.8318    0.6980       107\n",
      "         522     0.3907    0.6556    0.4896        90\n",
      "         532     0.4507    0.7273    0.5565        44\n",
      "          59     0.6299    0.7339    0.6780       109\n",
      "         601     0.5472    0.8169    0.6554        71\n",
      "         611     0.7890    0.9348    0.8557        92\n",
      "         617     0.5600    0.7368    0.6364        38\n",
      "         639     0.3061    0.5357    0.3896        28\n",
      "         668     0.1167    0.1556    0.1333        45\n",
      "         732     0.2115    0.4490    0.2876        98\n",
      "          74     0.1322    0.3239    0.1878        71\n",
      "         755     0.2083    0.5357    0.3000        28\n",
      "          77     0.4600    0.4662    0.4631       148\n",
      "         770     0.3000    0.3051    0.3025        59\n",
      "         772     0.4043    0.5588    0.4691        34\n",
      "          78     0.5090    0.7669    0.6119       296\n",
      "         787     0.7471    0.6573    0.6993       890\n",
      "          79     0.9940    0.7332    0.8439      2256\n",
      "         798     0.8286    0.6744    0.7436       129\n",
      "         835     0.5085    0.7317    0.6000        41\n",
      "         843     0.8710    0.7941    0.8308        34\n",
      "         862     0.6389    0.6273    0.6330       220\n",
      "         863     0.4059    0.3475    0.3744       118\n",
      "          89     0.9948    0.7952    0.8839       957\n",
      "         908     0.3784    0.5833    0.4590        24\n",
      "         918     0.5746    0.8556    0.6875        90\n",
      "          94     0.4956    0.5788    0.5340       292\n",
      "\n",
      "    accuracy                         0.6467     14202\n",
      "   macro avg     0.5209    0.6003    0.5453     14202\n",
      "weighted avg     0.6921    0.6467    0.6515     14202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_core_contextual_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_core_contextual_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
