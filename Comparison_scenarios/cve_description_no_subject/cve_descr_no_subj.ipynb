{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "260/260 [==============================] - 1s 1ms/step loss: 2.1129 - a\n",
      "Epoch 1 - F1 Score: 0.5187\n",
      "Saved best model\n",
      "[0.518681627990273]\n",
      "2336/2336 [==============================] - 11s 4ms/step - loss: 2.1129 - accuracy: 0.4432 - val_loss: 1.6214 - val_accuracy: 0.5508\n",
      "Epoch 2/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.52\n",
      "Epoch 2 - F1 Score: 0.5553\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.5282 - accuracy: 0.5729 - val_loss: 1.4813 - val_accuracy: 0.5829\n",
      "Epoch 3/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.405\n",
      "Epoch 3 - F1 Score: 0.5870\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.4050 - accuracy: 0.6035 - val_loss: 1.4082 - val_accuracy: 0.6073\n",
      "Epoch 4/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.3344\n",
      "Epoch 4 - F1 Score: 0.6051\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.3340 - accuracy: 0.6219 - val_loss: 1.3552 - val_accuracy: 0.6209\n",
      "Epoch 5/60\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "Epoch 5 - F1 Score: 0.6063\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.2841 - accuracy: 0.6348 - val_loss: 1.3384 - val_accuracy: 0.6236\n",
      "Epoch 6/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 1.2448 \n",
      "Epoch 6 - F1 Score: 0.6146\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.2448 - accuracy: 0.6446 - val_loss: 1.3332 - val_accuracy: 0.6250\n",
      "Epoch 7/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.2093 - accu\n",
      "Epoch 7 - F1 Score: 0.6213\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765]\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 1.2093 - accuracy: 0.6530 - val_loss: 1.3123 - val_accuracy: 0.6312\n",
      "Epoch 8/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1825 - accu\n",
      "Epoch 8 - F1 Score: 0.6242\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765, 0.6241554284910042]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.1821 - accuracy: 0.6593 - val_loss: 1.3007 - val_accuracy: 0.6374\n",
      "Epoch 9/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1545 - \n",
      "Epoch 9 - F1 Score: 0.6292\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765, 0.6241554284910042, 0.6292418771341655]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.1545 - accuracy: 0.6652 - val_loss: 1.2890 - val_accuracy: 0.6396\n",
      "Epoch 10/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 1.1286 - \n",
      "Epoch 10 - F1 Score: 0.6344\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765, 0.6241554284910042, 0.6292418771341655, 0.6343562323546592]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.1285 - accuracy: 0.6727 - val_loss: 1.2895 - val_accuracy: 0.6411\n",
      "Epoch 11/60\n",
      "260/260 [==============================] - 1s 3ms/step loss:\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.1071 - accuracy: 0.6768 - val_loss: 1.3017 - val_accuracy: 0.6363\n",
      "Epoch 12/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.08\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0838 - accuracy: 0.6826 - val_loss: 1.2808 - val_accuracy: 0.6418\n",
      "Epoch 13/60\n",
      "260/260 [==============================] - 1s 3ms/step loss: \n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 1.0624 - accuracy: 0.6891 - val_loss: 1.2912 - val_accuracy: 0.6405\n",
      "Epoch 14/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.0\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0433 - accuracy: 0.6946 - val_loss: 1.2993 - val_accuracy: 0.6409\n",
      "Epoch 15/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.0239 - ac\n",
      "Epoch 15 - F1 Score: 0.6384\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765, 0.6241554284910042, 0.6292418771341655, 0.6343562323546592, 0.628983015137121, 0.6325551874166615, 0.6338418336881713, 0.6305502770889684, 0.6384130212712096]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.0238 - accuracy: 0.6983 - val_loss: 1.2878 - val_accuracy: 0.6472\n",
      "Epoch 16/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 1.0063 \n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 1.0063 - accuracy: 0.7020 - val_loss: 1.3079 - val_accuracy: 0.6337\n",
      "Epoch 17/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9891 \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.9894 - accuracy: 0.7071 - val_loss: 1.3076 - val_accuracy: 0.6398\n",
      "Epoch 18/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.971\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.9711 - accuracy: 0.7123 - val_loss: 1.2935 - val_accuracy: 0.6430\n",
      "Epoch 19/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9545 \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.9548 - accuracy: 0.7165 - val_loss: 1.3212 - val_accuracy: 0.6345\n",
      "Epoch 20/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9384 \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.9376 - accuracy: 0.7218 - val_loss: 1.3190 - val_accuracy: 0.6460\n",
      "Epoch 21/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9226 \n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.9226 - accuracy: 0.7230 - val_loss: 1.3248 - val_accuracy: 0.6419\n",
      "Epoch 22/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9078 - ac\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.9079 - accuracy: 0.7277 - val_loss: 1.3290 - val_accuracy: 0.6440\n",
      "Epoch 23/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8933 - \n",
      "Epoch 23 - F1 Score: 0.6441\n",
      "Saved best model\n",
      "[0.518681627990273, 0.5552916137714813, 0.5870459488251618, 0.6051468019408144, 0.6063045950017564, 0.6145737979165472, 0.6213157631515765, 0.6241554284910042, 0.6292418771341655, 0.6343562323546592, 0.628983015137121, 0.6325551874166615, 0.6338418336881713, 0.6305502770889684, 0.6384130212712096, 0.6275092690194504, 0.6333563796819586, 0.6366793583818993, 0.6292850500676946, 0.6374252112535229, 0.6344011066871007, 0.6371261125627633, 0.6441116875612961]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.8929 - accuracy: 0.7316 - val_loss: 1.3258 - val_accuracy: 0.6480\n",
      "Epoch 24/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.8807 - \n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.8811 - accuracy: 0.7358 - val_loss: 1.3371 - val_accuracy: 0.6423\n",
      "Epoch 25/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8649 - ac\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.8650 - accuracy: 0.7402 - val_loss: 1.3475 - val_accuracy: 0.6419\n",
      "Epoch 26/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.851\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.8521 - accuracy: 0.7439 - val_loss: 1.3656 - val_accuracy: 0.6424\n",
      "Epoch 27/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.83\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8377 - accuracy: 0.7477 - val_loss: 1.3784 - val_accuracy: 0.6409\n",
      "Epoch 28/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.825\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8255 - accuracy: 0.7503 - val_loss: 1.3705 - val_accuracy: 0.6454\n",
      "Epoch 29/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8113 - accuracy: 0.7547 - val_loss: 1.3923 - val_accuracy: 0.6386\n",
      "Epoch 30/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.80\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8002 - accuracy: 0.7568 - val_loss: 1.4231 - val_accuracy: 0.6352\n",
      "Epoch 31/60\n",
      "260/260 [==============================] - 1s 2ms/step loss:\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.7884 - accuracy: 0.7607 - val_loss: 1.4354 - val_accuracy: 0.6365\n",
      "Epoch 32/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7750 - accuracy: 0.7641 - val_loss: 1.4401 - val_accuracy: 0.6336\n",
      "Epoch 33/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7646 - accuracy: 0.7654 - val_loss: 1.4442 - val_accuracy: 0.6354\n",
      "Epoch 34/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7512 - accuracy: 0.7704 - val_loss: 1.4577 - val_accuracy: 0.6303\n",
      "Epoch 35/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7408\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7409 - accuracy: 0.7725 - val_loss: 1.4871 - val_accuracy: 0.6329\n",
      "Epoch 36/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7298 - accuracy: 0.7765 - val_loss: 1.4556 - val_accuracy: 0.6378\n",
      "Epoch 37/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7194 - ac\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7197 - accuracy: 0.7772 - val_loss: 1.4823 - val_accuracy: 0.6357\n",
      "Epoch 38/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7092 - accuracy: 0.7818 - val_loss: 1.4993 - val_accuracy: 0.6351\n",
      "Epoch 39/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.6979 -\n",
      "2336/2336 [==============================] - 134s 57ms/step - loss: 0.6979 - accuracy: 0.7846 - val_loss: 1.5405 - val_accuracy: 0.6336\n",
      "Epoch 40/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.6892 - accuracy: 0.7871 - val_loss: 1.5244 - val_accuracy: 0.6293\n",
      "Epoch 41/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.6\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.6802 - accuracy: 0.7882 - val_loss: 1.5415 - val_accuracy: 0.6382\n",
      "Epoch 42/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.668\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.6682 - accuracy: 0.7934 - val_loss: 1.5478 - val_accuracy: 0.6334\n",
      "Epoch 43/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.65\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6582 - accuracy: 0.7961 - val_loss: 1.5846 - val_accuracy: 0.6318\n",
      "Epoch 44/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.650\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6498 - accuracy: 0.7983 - val_loss: 1.6359 - val_accuracy: 0.6236\n",
      "Epoch 45/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.642\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6427 - accuracy: 0.7998 - val_loss: 1.6014 - val_accuracy: 0.6317\n",
      "Epoch 46/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.632\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6319 - accuracy: 0.8028 - val_loss: 1.6368 - val_accuracy: 0.6284\n",
      "Epoch 47/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.621\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6217 - accuracy: 0.8065 - val_loss: 1.6519 - val_accuracy: 0.6336\n",
      "Epoch 48/60\n",
      "260/260 [==============================] - 1s 3ms/step loss:\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6153 - accuracy: 0.8079 - val_loss: 1.6682 - val_accuracy: 0.6359\n",
      "Epoch 49/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.60\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.6070 - accuracy: 0.8110 - val_loss: 1.6773 - val_accuracy: 0.6266\n",
      "Epoch 50/60\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.5995 \n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.5995 - accuracy: 0.8120 - val_loss: 1.6905 - val_accuracy: 0.6303\n",
      "Epoch 51/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.5888 - \n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.5888 - accuracy: 0.8150 - val_loss: 1.7104 - val_accuracy: 0.6251\n",
      "Epoch 52/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.5816 - \n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.5814 - accuracy: 0.8170 - val_loss: 1.7354 - val_accuracy: 0.6297\n",
      "Epoch 53/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.5732 - accuracy: 0.8196 - val_loss: 1.7360 - val_accuracy: 0.6312\n",
      "Epoch 54/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.5665 - \n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.5664 - accuracy: 0.8210 - val_loss: 1.7914 - val_accuracy: 0.6263\n",
      "Epoch 55/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.5590\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.5590 - accuracy: 0.8232 - val_loss: 1.8255 - val_accuracy: 0.6211\n",
      "Epoch 56/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.5\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.5508 - accuracy: 0.8267 - val_loss: 1.8067 - val_accuracy: 0.6274\n",
      "Epoch 57/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: \n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.5429 - accuracy: 0.8289 - val_loss: 1.8421 - val_accuracy: 0.6198\n",
      "Epoch 58/60\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.53\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.5358 - accuracy: 0.8314 - val_loss: 1.8625 - val_accuracy: 0.6198\n",
      "Epoch 59/60\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.5284 - ac\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.5288 - accuracy: 0.8329 - val_loss: 1.8527 - val_accuracy: 0.6235\n",
      "Epoch 60/60\n",
      "260/260 [==============================] - 1s 3ms/step loss\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.5209 - accuracy: 0.8351 - val_loss: 1.9017 - val_accuracy: 0.6210\n",
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7319    0.3598    0.4825      1070\n",
      "         120     0.3333    0.5969    0.4278       196\n",
      "         125     0.7591    0.7876    0.7731       532\n",
      "         134     0.3030    0.5263    0.3846        19\n",
      "         189     0.3958    0.3193    0.3535       119\n",
      "         190     0.5565    0.6900    0.6161       200\n",
      "          20     0.3703    0.1938    0.2545       810\n",
      "         200     0.6675    0.4390    0.5297       590\n",
      "         203     0.3696    0.6296    0.4658        27\n",
      "          22     0.7500    0.7645    0.7572       518\n",
      "         254     0.1905    0.1176    0.1455        34\n",
      "         255     0.3333    0.3582    0.3453        67\n",
      "         264     0.6136    0.4294    0.5053       503\n",
      "         269     0.1822    0.3868    0.2477       106\n",
      "         276     0.2373    0.2188    0.2276        64\n",
      "         284     0.2589    0.2358    0.2468       123\n",
      "         287     0.4041    0.5474    0.4650       285\n",
      "         295     0.4947    0.5802    0.5341        81\n",
      "         306     0.2991    0.3723    0.3318        94\n",
      "         310     0.7331    0.7390    0.7360       249\n",
      "         312     0.1522    0.1667    0.1591        42\n",
      "         319     0.3659    0.5882    0.4511        51\n",
      "         326     0.2083    0.1613    0.1818        31\n",
      "         327     0.2759    0.2286    0.2500        35\n",
      "         345     0.0000    0.0000    0.0000        26\n",
      "         347     0.2581    0.3333    0.2909        24\n",
      "         352     0.5191    0.7792    0.6231       453\n",
      "         362     0.4106    0.5082    0.4542       122\n",
      "         399     0.3465    0.6374    0.4489       262\n",
      "         400     0.2376    0.4855    0.3190       138\n",
      "         401     0.2621    0.5192    0.3484        52\n",
      "         415     0.4706    0.5714    0.5161        42\n",
      "         416     0.5510    0.7477    0.6345       325\n",
      "         426     0.5641    0.5238    0.5432        42\n",
      "         427     0.4545    0.6818    0.5455        44\n",
      "         434     0.5322    0.8093    0.6421       194\n",
      "         476     0.4901    0.7803    0.6021       223\n",
      "         502     0.4679    0.6822    0.5551       107\n",
      "         522     0.3571    0.5000    0.4167        90\n",
      "         532     0.3690    0.7045    0.4844        44\n",
      "          59     0.7143    0.6881    0.7009       109\n",
      "         601     0.4571    0.6761    0.5455        71\n",
      "         611     0.5556    0.7065    0.6220        92\n",
      "         617     0.4490    0.5789    0.5057        38\n",
      "         639     0.2000    0.3929    0.2651        28\n",
      "         668     0.0734    0.1778    0.1039        45\n",
      "         732     0.3253    0.2755    0.2983        98\n",
      "          74     0.0848    0.3944    0.1397        71\n",
      "         755     0.1806    0.4643    0.2600        28\n",
      "          77     0.3333    0.5203    0.4063       148\n",
      "         770     0.2333    0.3559    0.2819        59\n",
      "         772     0.2903    0.2647    0.2769        34\n",
      "          78     0.5300    0.6858    0.5979       296\n",
      "         787     0.6562    0.5854    0.6188       890\n",
      "          79     0.9767    0.4637    0.6288      2256\n",
      "         798     0.7434    0.6512    0.6942       129\n",
      "         835     0.4808    0.6098    0.5376        41\n",
      "         843     0.4545    0.4412    0.4478        34\n",
      "         862     0.4066    0.6727    0.5068       220\n",
      "         863     0.2302    0.2458    0.2377       118\n",
      "          89     0.9860    0.5883    0.7369       957\n",
      "         908     0.3864    0.7083    0.5000        24\n",
      "         918     0.3704    0.5556    0.4444        90\n",
      "          94     0.2301    0.5548    0.3253       292\n",
      "\n",
      "    accuracy                         0.5271     14202\n",
      "   macro avg     0.4160    0.4994    0.4372     14202\n",
      "weighted avg     0.6232    0.5271    0.5402     14202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test4.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_descr_no_subj_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_no_subject_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_no_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7319    0.3598    0.4825      1070\n",
      "         120     0.3333    0.5969    0.4278       196\n",
      "         125     0.7591    0.7876    0.7731       532\n",
      "         134     0.3030    0.5263    0.3846        19\n",
      "         189     0.3958    0.3193    0.3535       119\n",
      "         190     0.5565    0.6900    0.6161       200\n",
      "          20     0.3703    0.1938    0.2545       810\n",
      "         200     0.6675    0.4390    0.5297       590\n",
      "         203     0.3696    0.6296    0.4658        27\n",
      "          22     0.7500    0.7645    0.7572       518\n",
      "         254     0.1905    0.1176    0.1455        34\n",
      "         255     0.3333    0.3582    0.3453        67\n",
      "         264     0.6136    0.4294    0.5053       503\n",
      "         269     0.1822    0.3868    0.2477       106\n",
      "         276     0.2373    0.2188    0.2276        64\n",
      "         284     0.2589    0.2358    0.2468       123\n",
      "         287     0.4041    0.5474    0.4650       285\n",
      "         295     0.4947    0.5802    0.5341        81\n",
      "         306     0.2991    0.3723    0.3318        94\n",
      "         310     0.7331    0.7390    0.7360       249\n",
      "         312     0.1522    0.1667    0.1591        42\n",
      "         319     0.3659    0.5882    0.4511        51\n",
      "         326     0.2083    0.1613    0.1818        31\n",
      "         327     0.2759    0.2286    0.2500        35\n",
      "         345     0.0000    0.0000    0.0000        26\n",
      "         347     0.2581    0.3333    0.2909        24\n",
      "         352     0.5191    0.7792    0.6231       453\n",
      "         362     0.4106    0.5082    0.4542       122\n",
      "         399     0.3465    0.6374    0.4489       262\n",
      "         400     0.2376    0.4855    0.3190       138\n",
      "         401     0.2621    0.5192    0.3484        52\n",
      "         415     0.4706    0.5714    0.5161        42\n",
      "         416     0.5510    0.7477    0.6345       325\n",
      "         426     0.5641    0.5238    0.5432        42\n",
      "         427     0.4545    0.6818    0.5455        44\n",
      "         434     0.5322    0.8093    0.6421       194\n",
      "         476     0.4901    0.7803    0.6021       223\n",
      "         502     0.4679    0.6822    0.5551       107\n",
      "         522     0.3571    0.5000    0.4167        90\n",
      "         532     0.3690    0.7045    0.4844        44\n",
      "          59     0.7143    0.6881    0.7009       109\n",
      "         601     0.4571    0.6761    0.5455        71\n",
      "         611     0.5556    0.7065    0.6220        92\n",
      "         617     0.4490    0.5789    0.5057        38\n",
      "         639     0.2000    0.3929    0.2651        28\n",
      "         668     0.0734    0.1778    0.1039        45\n",
      "         732     0.3253    0.2755    0.2983        98\n",
      "          74     0.0848    0.3944    0.1397        71\n",
      "         755     0.1806    0.4643    0.2600        28\n",
      "          77     0.3333    0.5203    0.4063       148\n",
      "         770     0.2333    0.3559    0.2819        59\n",
      "         772     0.2903    0.2647    0.2769        34\n",
      "          78     0.5300    0.6858    0.5979       296\n",
      "         787     0.6562    0.5854    0.6188       890\n",
      "          79     0.9767    0.4637    0.6288      2256\n",
      "         798     0.7434    0.6512    0.6942       129\n",
      "         835     0.4808    0.6098    0.5376        41\n",
      "         843     0.4545    0.4412    0.4478        34\n",
      "         862     0.4066    0.6727    0.5068       220\n",
      "         863     0.2302    0.2458    0.2377       118\n",
      "          89     0.9860    0.5883    0.7369       957\n",
      "         908     0.3864    0.7083    0.5000        24\n",
      "         918     0.3704    0.5556    0.4444        90\n",
      "          94     0.2301    0.5548    0.3253       292\n",
      "\n",
      "    accuracy                         0.5271     14202\n",
      "   macro avg     0.4160    0.4994    0.4372     14202\n",
      "weighted avg     0.6232    0.5271    0.5402     14202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_descr_no_subj_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_no_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
