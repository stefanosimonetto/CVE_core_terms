{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.8803 - accur\n",
      "Epoch 1 - F1 Score: 0.5846\n",
      "Saved best model\n",
      "[0.5846069226494558]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.8783 - accuracy: 0.5031 - val_loss: 1.4178 - val_accuracy: 0.6076\n",
      "Epoch 2/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.3\n",
      "Epoch 2 - F1 Score: 0.6252\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.3175 - accuracy: 0.6293 - val_loss: 1.2906 - val_accuracy: 0.6380\n",
      "Epoch 3/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.2\n",
      "Epoch 3 - F1 Score: 0.6310\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717]\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 1.2113 - accuracy: 0.6553 - val_loss: 1.2355 - val_accuracy: 0.6459\n",
      "Epoch 4/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 4 - F1 Score: 0.6424\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.1539 - accuracy: 0.6712 - val_loss: 1.2109 - val_accuracy: 0.6542\n",
      "Epoch 5/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.1\n",
      "Epoch 5 - F1 Score: 0.6602\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155]\n",
      "2336/2336 [==============================] - 11s 5ms/step - loss: 1.1175 - accuracy: 0.6780 - val_loss: 1.1782 - val_accuracy: 0.6726\n",
      "Epoch 6/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.0881 - accuracy: 0.6872 - val_loss: 1.1505 - val_accuracy: 0.6712\n",
      "Epoch 7/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.0638 - a\n",
      "Epoch 7 - F1 Score: 0.6654\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052]\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 1.0640 - accuracy: 0.6921 - val_loss: 1.1463 - val_accuracy: 0.6749\n",
      "Epoch 8/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.04\n",
      "Epoch 8 - F1 Score: 0.6731\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0443 - accuracy: 0.6971 - val_loss: 1.1380 - val_accuracy: 0.6792\n",
      "Epoch 9/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.0251\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 1.0250 - accuracy: 0.7015 - val_loss: 1.1336 - val_accuracy: 0.6790\n",
      "Epoch 10/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 1.0096 \n",
      "Epoch 10 - F1 Score: 0.6737\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.0096 - accuracy: 0.7056 - val_loss: 1.1351 - val_accuracy: 0.6820\n",
      "Epoch 11/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9939 - a\n",
      "Epoch 11 - F1 Score: 0.6771\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908, 0.6771489784999803]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9933 - accuracy: 0.7078 - val_loss: 1.1232 - val_accuracy: 0.6871\n",
      "Epoch 12/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9791 - accuracy: 0.7119 - val_loss: 1.1292 - val_accuracy: 0.6847\n",
      "Epoch 13/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9658 - a\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.9658 - accuracy: 0.7153 - val_loss: 1.1301 - val_accuracy: 0.6823\n",
      "Epoch 14/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9545 -\n",
      "Epoch 14 - F1 Score: 0.6806\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908, 0.6771489784999803, 0.6766067131882848, 0.6750242699364271, 0.6806075716089716]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9542 - accuracy: 0.7182 - val_loss: 1.1053 - val_accuracy: 0.6898\n",
      "Epoch 15/40\n",
      "260/260 [==============================] - 1s 2ms/step loss:\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9401 - accuracy: 0.7208 - val_loss: 1.1205 - val_accuracy: 0.6863\n",
      "Epoch 16/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.92\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9288 - accuracy: 0.7235 - val_loss: 1.1431 - val_accuracy: 0.6764\n",
      "Epoch 17/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.91\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.9162 - accuracy: 0.7271 - val_loss: 1.1324 - val_accuracy: 0.6822\n",
      "Epoch 18/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9051\n",
      "Epoch 18 - F1 Score: 0.6858\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908, 0.6771489784999803, 0.6766067131882848, 0.6750242699364271, 0.6806075716089716, 0.6765202846090846, 0.6653939470003293, 0.6759401316958052, 0.6858255342409699]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.9050 - accuracy: 0.7292 - val_loss: 1.1135 - val_accuracy: 0.6878\n",
      "Epoch 19/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8929 -\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8931 - accuracy: 0.7323 - val_loss: 1.1197 - val_accuracy: 0.6893\n",
      "Epoch 20/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.882\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8820 - accuracy: 0.7343 - val_loss: 1.1149 - val_accuracy: 0.6912\n",
      "Epoch 21/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.87\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.8725 - accuracy: 0.7379 - val_loss: 1.1269 - val_accuracy: 0.6877\n",
      "Epoch 22/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.86\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8622 - accuracy: 0.7403 - val_loss: 1.1339 - val_accuracy: 0.6847\n",
      "Epoch 23/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8526\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8529 - accuracy: 0.7419 - val_loss: 1.1235 - val_accuracy: 0.6895\n",
      "Epoch 24/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.8415 - a\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.8422 - accuracy: 0.7456 - val_loss: 1.1327 - val_accuracy: 0.6906\n",
      "Epoch 25/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8323 -\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8330 - accuracy: 0.7482 - val_loss: 1.1496 - val_accuracy: 0.6866\n",
      "Epoch 26/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8239 - a\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.8239 - accuracy: 0.7502 - val_loss: 1.1373 - val_accuracy: 0.6871\n",
      "Epoch 27/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8140\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.8137 - accuracy: 0.7529 - val_loss: 1.1553 - val_accuracy: 0.6873\n",
      "Epoch 28/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.80\n",
      "Epoch 28 - F1 Score: 0.6862\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908, 0.6771489784999803, 0.6766067131882848, 0.6750242699364271, 0.6806075716089716, 0.6765202846090846, 0.6653939470003293, 0.6759401316958052, 0.6858255342409699, 0.6820019323538059, 0.6850387168177869, 0.6811818568216678, 0.6799508966292365, 0.684894380370542, 0.6852796903319083, 0.6824407435188047, 0.6837705694706184, 0.6815830920162846, 0.6861551342037878]\n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.8054 - accuracy: 0.7551 - val_loss: 1.1583 - val_accuracy: 0.6898\n",
      "Epoch 29/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7964 - a\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7964 - accuracy: 0.7574 - val_loss: 1.1715 - val_accuracy: 0.6878\n",
      "Epoch 30/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7883 - \n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7883 - accuracy: 0.7590 - val_loss: 1.1558 - val_accuracy: 0.6876\n",
      "Epoch 31/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.7780 - a\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7780 - accuracy: 0.7608 - val_loss: 1.1555 - val_accuracy: 0.6904\n",
      "Epoch 32/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7706\n",
      "Epoch 32 - F1 Score: 0.6872\n",
      "Saved best model\n",
      "[0.5846069226494558, 0.6251923115572853, 0.6310399700337717, 0.64244596896992, 0.6601530959076155, 0.6565380779015124, 0.6654016240437052, 0.6731335784250614, 0.6681462295594583, 0.6737206041538908, 0.6771489784999803, 0.6766067131882848, 0.6750242699364271, 0.6806075716089716, 0.6765202846090846, 0.6653939470003293, 0.6759401316958052, 0.6858255342409699, 0.6820019323538059, 0.6850387168177869, 0.6811818568216678, 0.6799508966292365, 0.684894380370542, 0.6852796903319083, 0.6824407435188047, 0.6837705694706184, 0.6815830920162846, 0.6861551342037878, 0.6835180878703306, 0.6838046216965087, 0.6856282211988846, 0.6871673868577085]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 0.7709 - accuracy: 0.7634 - val_loss: 1.1617 - val_accuracy: 0.6904\n",
      "Epoch 33/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7623 -\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7625 - accuracy: 0.7660 - val_loss: 1.1662 - val_accuracy: 0.6885\n",
      "Epoch 34/40\n",
      "260/260 [==============================] - 1s 1ms/step loss: 0.7547 - a\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7546 - accuracy: 0.7679 - val_loss: 1.1794 - val_accuracy: 0.6881\n",
      "Epoch 35/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.7438 -\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7439 - accuracy: 0.7696 - val_loss: 1.1902 - val_accuracy: 0.6848\n",
      "Epoch 36/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7368\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7369 - accuracy: 0.7735 - val_loss: 1.2246 - val_accuracy: 0.6817\n",
      "Epoch 37/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.72\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7295 - accuracy: 0.7748 - val_loss: 1.2028 - val_accuracy: 0.6840\n",
      "Epoch 38/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7218 \n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.7225 - accuracy: 0.7767 - val_loss: 1.2244 - val_accuracy: 0.6820\n",
      "Epoch 39/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.7135 - a\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 0.7135 - accuracy: 0.7797 - val_loss: 1.2214 - val_accuracy: 0.6843\n",
      "Epoch 40/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.7054 - acc\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.7054 - accuracy: 0.7821 - val_loss: 1.2284 - val_accuracy: 0.6849\n",
      "444/444 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8414    0.4561    0.5915      1070\n",
      "         120     0.3504    0.8724    0.5000       196\n",
      "         125     0.7726    0.8365    0.8032       532\n",
      "         134     0.8095    0.8947    0.8500        19\n",
      "         189     0.6098    0.6303    0.6198       119\n",
      "         190     0.6803    0.8300    0.7477       200\n",
      "          20     0.4444    0.1827    0.2590       810\n",
      "         200     0.5622    0.5746    0.5683       590\n",
      "         203     0.6061    0.7407    0.6667        27\n",
      "          22     0.8609    0.8958    0.8780       518\n",
      "         254     0.1333    0.1176    0.1250        34\n",
      "         255     0.3247    0.3731    0.3472        67\n",
      "         264     0.5304    0.4851    0.5067       503\n",
      "         269     0.3431    0.4434    0.3868       106\n",
      "         276     0.3111    0.2188    0.2569        64\n",
      "         284     0.2478    0.2276    0.2373       123\n",
      "         287     0.5287    0.6456    0.5814       285\n",
      "         295     0.6438    0.5802    0.6104        81\n",
      "         306     0.4091    0.3830    0.3956        94\n",
      "         310     0.7168    0.8032    0.7576       249\n",
      "         312     0.2368    0.2143    0.2250        42\n",
      "         319     0.4483    0.5098    0.4771        51\n",
      "         326     0.4615    0.1935    0.2727        31\n",
      "         327     0.3793    0.3143    0.3438        35\n",
      "         345     0.1714    0.2308    0.1967        26\n",
      "         347     0.3256    0.5833    0.4179        24\n",
      "         352     0.7174    0.9470    0.8164       453\n",
      "         362     0.7239    0.7951    0.7578       122\n",
      "         399     0.3507    0.5382    0.4247       262\n",
      "         400     0.4322    0.3696    0.3984       138\n",
      "         401     0.4355    0.5192    0.4737        52\n",
      "         415     0.5000    0.9286    0.6500        42\n",
      "         416     0.8214    0.8492    0.8351       325\n",
      "         426     0.7576    0.5952    0.6667        42\n",
      "         427     0.5286    0.8409    0.6491        44\n",
      "         434     0.4574    0.8866    0.6035       194\n",
      "         476     0.7176    0.8206    0.7657       223\n",
      "         502     0.6589    0.7944    0.7203       107\n",
      "         522     0.4720    0.6556    0.5488        90\n",
      "         532     0.3368    0.7273    0.4604        44\n",
      "          59     0.5610    0.8440    0.6740       109\n",
      "         601     0.6548    0.7746    0.7097        71\n",
      "         611     0.5752    0.9565    0.7184        92\n",
      "         617     0.2703    0.7895    0.4027        38\n",
      "         639     0.7143    0.5357    0.6122        28\n",
      "         668     0.0938    0.1333    0.1101        45\n",
      "         732     0.2920    0.4082    0.3404        98\n",
      "          74     0.1227    0.3803    0.1856        71\n",
      "         755     0.2593    0.5000    0.3415        28\n",
      "          77     0.6353    0.3649    0.4635       148\n",
      "         770     0.3049    0.4237    0.3546        59\n",
      "         772     0.3091    0.5000    0.3820        34\n",
      "          78     0.5135    0.7736    0.6173       296\n",
      "         787     0.7467    0.6888    0.7165       890\n",
      "          79     0.9943    0.6990    0.8209      2256\n",
      "         798     0.8036    0.6977    0.7469       129\n",
      "         835     0.5323    0.8049    0.6408        41\n",
      "         843     0.8529    0.8529    0.8529        34\n",
      "         862     0.4507    0.6864    0.5441       220\n",
      "         863     0.3082    0.3814    0.3409       118\n",
      "          89     0.9945    0.7544    0.8580       957\n",
      "         908     0.4483    0.5417    0.4906        24\n",
      "         918     0.8061    0.8778    0.8404        90\n",
      "          94     0.4861    0.5993    0.5368       292\n",
      "\n",
      "    accuracy                         0.6412     14202\n",
      "   macro avg     0.5217    0.6011    0.5421     14202\n",
      "weighted avg     0.6919    0.6412    0.6459     14202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test4.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_terms_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_terms_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8414    0.4561    0.5915      1070\n",
      "         120     0.3504    0.8724    0.5000       196\n",
      "         125     0.7726    0.8365    0.8032       532\n",
      "         134     0.8095    0.8947    0.8500        19\n",
      "         189     0.6098    0.6303    0.6198       119\n",
      "         190     0.6803    0.8300    0.7477       200\n",
      "          20     0.4444    0.1827    0.2590       810\n",
      "         200     0.5622    0.5746    0.5683       590\n",
      "         203     0.6061    0.7407    0.6667        27\n",
      "          22     0.8609    0.8958    0.8780       518\n",
      "         254     0.1333    0.1176    0.1250        34\n",
      "         255     0.3247    0.3731    0.3472        67\n",
      "         264     0.5304    0.4851    0.5067       503\n",
      "         269     0.3431    0.4434    0.3868       106\n",
      "         276     0.3111    0.2188    0.2569        64\n",
      "         284     0.2478    0.2276    0.2373       123\n",
      "         287     0.5287    0.6456    0.5814       285\n",
      "         295     0.6438    0.5802    0.6104        81\n",
      "         306     0.4091    0.3830    0.3956        94\n",
      "         310     0.7168    0.8032    0.7576       249\n",
      "         312     0.2368    0.2143    0.2250        42\n",
      "         319     0.4483    0.5098    0.4771        51\n",
      "         326     0.4615    0.1935    0.2727        31\n",
      "         327     0.3793    0.3143    0.3438        35\n",
      "         345     0.1714    0.2308    0.1967        26\n",
      "         347     0.3256    0.5833    0.4179        24\n",
      "         352     0.7174    0.9470    0.8164       453\n",
      "         362     0.7239    0.7951    0.7578       122\n",
      "         399     0.3507    0.5382    0.4247       262\n",
      "         400     0.4322    0.3696    0.3984       138\n",
      "         401     0.4355    0.5192    0.4737        52\n",
      "         415     0.5000    0.9286    0.6500        42\n",
      "         416     0.8214    0.8492    0.8351       325\n",
      "         426     0.7576    0.5952    0.6667        42\n",
      "         427     0.5286    0.8409    0.6491        44\n",
      "         434     0.4574    0.8866    0.6035       194\n",
      "         476     0.7176    0.8206    0.7657       223\n",
      "         502     0.6589    0.7944    0.7203       107\n",
      "         522     0.4720    0.6556    0.5488        90\n",
      "         532     0.3368    0.7273    0.4604        44\n",
      "          59     0.5610    0.8440    0.6740       109\n",
      "         601     0.6548    0.7746    0.7097        71\n",
      "         611     0.5752    0.9565    0.7184        92\n",
      "         617     0.2703    0.7895    0.4027        38\n",
      "         639     0.7143    0.5357    0.6122        28\n",
      "         668     0.0938    0.1333    0.1101        45\n",
      "         732     0.2920    0.4082    0.3404        98\n",
      "          74     0.1227    0.3803    0.1856        71\n",
      "         755     0.2593    0.5000    0.3415        28\n",
      "          77     0.6353    0.3649    0.4635       148\n",
      "         770     0.3049    0.4237    0.3546        59\n",
      "         772     0.3091    0.5000    0.3820        34\n",
      "          78     0.5135    0.7736    0.6173       296\n",
      "         787     0.7467    0.6888    0.7165       890\n",
      "          79     0.9943    0.6990    0.8209      2256\n",
      "         798     0.8036    0.6977    0.7469       129\n",
      "         835     0.5323    0.8049    0.6408        41\n",
      "         843     0.8529    0.8529    0.8529        34\n",
      "         862     0.4507    0.6864    0.5441       220\n",
      "         863     0.3082    0.3814    0.3409       118\n",
      "          89     0.9945    0.7544    0.8580       957\n",
      "         908     0.4483    0.5417    0.4906        24\n",
      "         918     0.8061    0.8778    0.8404        90\n",
      "          94     0.4861    0.5993    0.5368       292\n",
      "\n",
      "    accuracy                         0.6412     14202\n",
      "   macro avg     0.5217    0.6011    0.5421     14202\n",
      "weighted avg     0.6919    0.6412    0.6459     14202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_terms_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
