{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 2.1536 -\n",
      "Epoch 1 - F1 Score: 0.4944\n",
      "Saved best model\n",
      "[0.4943675509814705]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 2.1518 - accuracy: 0.4332 - val_loss: 1.6853 - val_accuracy: 0.5358\n",
      "Epoch 2/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.557\n",
      "Epoch 2 - F1 Score: 0.5636\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905]\n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 1.5563 - accuracy: 0.5654 - val_loss: 1.5153 - val_accuracy: 0.5801\n",
      "Epoch 3/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 1.4315 \n",
      "Epoch 3 - F1 Score: 0.5682\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052]\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 1.4310 - accuracy: 0.5966 - val_loss: 1.4456 - val_accuracy: 0.5914\n",
      "Epoch 4/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 1.3581 \n",
      "Epoch 4 - F1 Score: 0.6030\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412]\n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 1.3581 - accuracy: 0.6169 - val_loss: 1.3640 - val_accuracy: 0.6168\n",
      "Epoch 5/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.307\n",
      "Epoch 5 - F1 Score: 0.6055\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.3067 - accuracy: 0.6288 - val_loss: 1.3430 - val_accuracy: 0.6240\n",
      "Epoch 6/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.267\n",
      "Epoch 6 - F1 Score: 0.6206\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.2676 - accuracy: 0.6371 - val_loss: 1.3027 - val_accuracy: 0.6329\n",
      "Epoch 7/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.2329\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.2329 - accuracy: 0.6456 - val_loss: 1.3030 - val_accuracy: 0.6327\n",
      "Epoch 8/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.202\n",
      "Epoch 8 - F1 Score: 0.6285\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.2023 - accuracy: 0.6544 - val_loss: 1.2924 - val_accuracy: 0.6400\n",
      "Epoch 9/60\n",
      "259/259 [==============================] - 1s 3ms/step loss: \n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 1.1755 - accuracy: 0.6600 - val_loss: 1.2728 - val_accuracy: 0.6401\n",
      "Epoch 10/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.\n",
      "Epoch 10 - F1 Score: 0.6364\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.1516 - accuracy: 0.6671 - val_loss: 1.2669 - val_accuracy: 0.6477\n",
      "Epoch 11/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.129\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.1292 - accuracy: 0.6704 - val_loss: 1.2718 - val_accuracy: 0.6421\n",
      "Epoch 12/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.10\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.1055 - accuracy: 0.6777 - val_loss: 1.2776 - val_accuracy: 0.6430\n",
      "Epoch 13/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.08\n",
      "Epoch 13 - F1 Score: 0.6381\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.0860 - accuracy: 0.6816 - val_loss: 1.2631 - val_accuracy: 0.6443\n",
      "Epoch 14/60\n",
      "259/259 [==============================] - 1s 3ms/step loss\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.0673 - accuracy: 0.6864 - val_loss: 1.2645 - val_accuracy: 0.6470\n",
      "Epoch 15/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.04\n",
      "Epoch 15 - F1 Score: 0.6390\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976, 0.6359459687884668, 0.6389606829519779]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.0475 - accuracy: 0.6911 - val_loss: 1.2666 - val_accuracy: 0.6482\n",
      "Epoch 16/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.030\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 1.0307 - accuracy: 0.6959 - val_loss: 1.2787 - val_accuracy: 0.6439\n",
      "Epoch 17/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 1.0107\n",
      "Epoch 17 - F1 Score: 0.6392\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976, 0.6359459687884668, 0.6389606829519779, 0.6366563434724296, 0.6391854686427121]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 1.0107 - accuracy: 0.7008 - val_loss: 1.2830 - val_accuracy: 0.6472\n",
      "Epoch 18/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.99\n",
      "Epoch 18 - F1 Score: 0.6409\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976, 0.6359459687884668, 0.6389606829519779, 0.6366563434724296, 0.6391854686427121, 0.64086426692995]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.9952 - accuracy: 0.7038 - val_loss: 1.2870 - val_accuracy: 0.6487\n",
      "Epoch 19/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.9792 -\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.9796 - accuracy: 0.7077 - val_loss: 1.2855 - val_accuracy: 0.6499\n",
      "Epoch 20/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.9655 - accuracy: 0.7121 - val_loss: 1.2808 - val_accuracy: 0.6488\n",
      "Epoch 21/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.9500 - \n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.9500 - accuracy: 0.7153 - val_loss: 1.3141 - val_accuracy: 0.6383\n",
      "Epoch 22/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.935\n",
      "2323/2323 [==============================] - 7s 3ms/step - loss: 0.9351 - accuracy: 0.7201 - val_loss: 1.3013 - val_accuracy: 0.6450\n",
      "Epoch 23/60\n",
      "259/259 [==============================] - 1s 3ms/step\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 0.9226 - accuracy: 0.7223 - val_loss: 1.2938 - val_accuracy: 0.6477\n",
      "Epoch 24/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.90\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.9065 - accuracy: 0.7272 - val_loss: 1.3151 - val_accuracy: 0.6470\n",
      "Epoch 25/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0\n",
      "Epoch 25 - F1 Score: 0.6433\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976, 0.6359459687884668, 0.6389606829519779, 0.6366563434724296, 0.6391854686427121, 0.64086426692995, 0.6400542981837384, 0.6378474889932657, 0.6300415462551976, 0.6378954252591934, 0.6398045796282904, 0.6387120087407347, 0.6433219953088422]\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.8944 - accuracy: 0.7312 - val_loss: 1.3275 - val_accuracy: 0.6501\n",
      "Epoch 26/60\n",
      "259/259 [==============================] - 1s 3ms/step loss:\n",
      "Epoch 26 - F1 Score: 0.6444\n",
      "Saved best model\n",
      "[0.4943675509814705, 0.5635767145664905, 0.568190693154052, 0.6029948725458412, 0.6055000069470833, 0.6205682425459038, 0.6195782749787672, 0.6284922539272343, 0.6278771403228898, 0.6364184470290849, 0.6307493011887688, 0.6311629648913846, 0.6380705243582976, 0.6359459687884668, 0.6389606829519779, 0.6366563434724296, 0.6391854686427121, 0.64086426692995, 0.6400542981837384, 0.6378474889932657, 0.6300415462551976, 0.6378954252591934, 0.6398045796282904, 0.6387120087407347, 0.6433219953088422, 0.644371345328763]\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.8833 - accuracy: 0.7347 - val_loss: 1.3342 - val_accuracy: 0.6478\n",
      "Epoch 27/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.8695\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 0.8697 - accuracy: 0.7388 - val_loss: 1.3314 - val_accuracy: 0.6477\n",
      "Epoch 28/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.858\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 0.8587 - accuracy: 0.7401 - val_loss: 1.3413 - val_accuracy: 0.6432\n",
      "Epoch 29/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2323/2323 [==============================] - 10s 5ms/step - loss: 0.8459 - accuracy: 0.7443 - val_loss: 1.3715 - val_accuracy: 0.6424\n",
      "Epoch 30/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.8346 - \n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.8350 - accuracy: 0.7468 - val_loss: 1.3603 - val_accuracy: 0.6458\n",
      "Epoch 31/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.8244 \n",
      "2323/2323 [==============================] - 8s 4ms/step - loss: 0.8243 - accuracy: 0.7497 - val_loss: 1.3559 - val_accuracy: 0.6439\n",
      "Epoch 32/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: \n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.8124 - accuracy: 0.7528 - val_loss: 1.3846 - val_accuracy: 0.6368\n",
      "Epoch 33/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.802\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.8024 - accuracy: 0.7563 - val_loss: 1.3930 - val_accuracy: 0.6426\n",
      "Epoch 34/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.79\n",
      "2323/2323 [==============================] - 8s 4ms/step - loss: 0.7919 - accuracy: 0.7601 - val_loss: 1.4104 - val_accuracy: 0.6356\n",
      "Epoch 35/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: \n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.7816 - accuracy: 0.7618 - val_loss: 1.4013 - val_accuracy: 0.6429\n",
      "Epoch 36/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.7732\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.7730 - accuracy: 0.7636 - val_loss: 1.4401 - val_accuracy: 0.6316\n",
      "Epoch 37/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.7622 - accuracy: 0.7671 - val_loss: 1.4399 - val_accuracy: 0.6357\n",
      "Epoch 38/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.753\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.7530 - accuracy: 0.7700 - val_loss: 1.4656 - val_accuracy: 0.6341\n",
      "Epoch 39/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.742\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.7421 - accuracy: 0.7724 - val_loss: 1.4913 - val_accuracy: 0.6293\n",
      "Epoch 40/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.732\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.7326 - accuracy: 0.7745 - val_loss: 1.4847 - val_accuracy: 0.6349\n",
      "Epoch 41/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.724\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.7249 - accuracy: 0.7767 - val_loss: 1.5012 - val_accuracy: 0.6351\n",
      "Epoch 42/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.7151 - \n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.7156 - accuracy: 0.7796 - val_loss: 1.4967 - val_accuracy: 0.6351\n",
      "Epoch 43/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.7059 - \n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 0.7058 - accuracy: 0.7826 - val_loss: 1.5351 - val_accuracy: 0.6317\n",
      "Epoch 44/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.6965 \n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 0.6970 - accuracy: 0.7848 - val_loss: 1.5072 - val_accuracy: 0.6338\n",
      "Epoch 45/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.688\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.6888 - accuracy: 0.7862 - val_loss: 1.5397 - val_accuracy: 0.6321\n",
      "Epoch 46/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.683\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.6833 - accuracy: 0.7884 - val_loss: 1.5389 - val_accuracy: 0.6345\n",
      "Epoch 47/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.6730 - accuracy: 0.7906 - val_loss: 1.5648 - val_accuracy: 0.6235\n",
      "Epoch 48/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.663\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.6639 - accuracy: 0.7944 - val_loss: 1.5896 - val_accuracy: 0.6311\n",
      "Epoch 49/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.\n",
      "2323/2323 [==============================] - 10s 4ms/step - loss: 0.6592 - accuracy: 0.7942 - val_loss: 1.5950 - val_accuracy: 0.6299\n",
      "Epoch 50/60\n",
      "259/259 [==============================] - 1s 2ms/step loss: 0.6\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.6492 - accuracy: 0.7974 - val_loss: 1.6075 - val_accuracy: 0.6266\n",
      "Epoch 51/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.6430 \n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 0.6433 - accuracy: 0.7988 - val_loss: 1.5926 - val_accuracy: 0.6271\n",
      "Epoch 52/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.6349 - ac\n",
      "2323/2323 [==============================] - 8s 3ms/step - loss: 0.6349 - accuracy: 0.8023 - val_loss: 1.6452 - val_accuracy: 0.6237\n",
      "Epoch 53/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.6258 - \n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.6259 - accuracy: 0.8027 - val_loss: 1.6692 - val_accuracy: 0.6241\n",
      "Epoch 54/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.6183 - ac\n",
      "2323/2323 [==============================] - 7s 3ms/step - loss: 0.6185 - accuracy: 0.8070 - val_loss: 1.6890 - val_accuracy: 0.6200\n",
      "Epoch 55/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.6135 - ac\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.6141 - accuracy: 0.8071 - val_loss: 1.6733 - val_accuracy: 0.6271\n",
      "Epoch 56/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.6052 - accu\n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.6059 - accuracy: 0.8088 - val_loss: 1.7071 - val_accuracy: 0.6189\n",
      "Epoch 57/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.5970 - accu\n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.5972 - accuracy: 0.8126 - val_loss: 1.7193 - val_accuracy: 0.6258\n",
      "Epoch 58/60\n",
      "259/259 [==============================] - 0s 2ms/step loss: 0.5921 - \n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.5923 - accuracy: 0.8131 - val_loss: 1.7429 - val_accuracy: 0.6201\n",
      "Epoch 59/60\n",
      "259/259 [==============================] - 0s 1ms/step loss: 0.5844 - accu\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.5848 - accuracy: 0.8151 - val_loss: 1.7605 - val_accuracy: 0.6245\n",
      "Epoch 60/60\n",
      "259/259 [==============================] - 1s 3ms/step\n",
      "2323/2323 [==============================] - 9s 4ms/step - loss: 0.5800 - accuracy: 0.8164 - val_loss: 1.7343 - val_accuracy: 0.6209\n",
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7550    0.3542    0.4822      1070\n",
      "         120     0.2665    0.6378    0.3759       196\n",
      "         125     0.7588    0.7688    0.7638       532\n",
      "         134     0.6364    0.7368    0.6829        19\n",
      "         189     0.3152    0.4874    0.3828       119\n",
      "         190     0.4651    0.7000    0.5589       200\n",
      "          20     0.4965    0.1765    0.2605       810\n",
      "         200     0.6638    0.3949    0.4952       590\n",
      "         203     0.3846    0.7407    0.5063        27\n",
      "          22     0.8016    0.7568    0.7786       518\n",
      "         254     0.1379    0.1176    0.1270        34\n",
      "         255     0.2929    0.4328    0.3494        67\n",
      "         264     0.5649    0.5368    0.5505       503\n",
      "         269     0.2647    0.3396    0.2975       106\n",
      "         276     0.1471    0.3125    0.2000        64\n",
      "         284     0.2970    0.2439    0.2679       123\n",
      "         287     0.4266    0.5509    0.4809       285\n",
      "         295     0.5165    0.5802    0.5465        81\n",
      "         306     0.3140    0.2872    0.3000        94\n",
      "         310     0.7792    0.7229    0.7500       249\n",
      "         312     0.1081    0.1905    0.1379        42\n",
      "         319     0.4074    0.4314    0.4190        51\n",
      "         326     0.2778    0.1613    0.2041        31\n",
      "         327     0.3226    0.2857    0.3030        35\n",
      "         345     0.1277    0.2308    0.1644        26\n",
      "         347     0.3871    0.5000    0.4364        24\n",
      "         352     0.4054    0.7991    0.5379       453\n",
      "         362     0.3830    0.4426    0.4106       122\n",
      "         399     0.3738    0.5992    0.4604       262\n",
      "         400     0.2947    0.4420    0.3536       138\n",
      "         401     0.3600    0.3462    0.3529        52\n",
      "         415     0.4576    0.6429    0.5347        42\n",
      "         416     0.5788    0.7231    0.6430       325\n",
      "         426     0.4355    0.6429    0.5192        42\n",
      "         427     0.3529    0.5455    0.4286        44\n",
      "         434     0.5627    0.7629    0.6477       194\n",
      "         476     0.4814    0.7534    0.5874       223\n",
      "         502     0.5208    0.7009    0.5976       107\n",
      "         522     0.2785    0.4889    0.3548        90\n",
      "         532     0.3269    0.7727    0.4595        44\n",
      "          59     0.6641    0.7982    0.7250       109\n",
      "         601     0.3714    0.7324    0.4929        71\n",
      "         611     0.3544    0.7935    0.4899        92\n",
      "         617     0.5938    0.5000    0.5429        38\n",
      "         639     0.1695    0.3571    0.2299        28\n",
      "         668     0.1026    0.0889    0.0952        45\n",
      "         732     0.2637    0.2449    0.2540        98\n",
      "          74     0.1224    0.3380    0.1798        71\n",
      "         755     0.1692    0.3929    0.2366        28\n",
      "          77     0.4337    0.4865    0.4586       148\n",
      "         770     0.2500    0.3898    0.3046        59\n",
      "         772     0.4737    0.5294    0.5000        34\n",
      "          78     0.5134    0.7095    0.5957       296\n",
      "         787     0.6710    0.5225    0.5875       890\n",
      "          79     0.9828    0.4295    0.5978      2256\n",
      "         798     0.7455    0.6357    0.6862       129\n",
      "         835     0.4688    0.7317    0.5714        41\n",
      "         843     0.3500    0.4118    0.3784        34\n",
      "         862     0.4560    0.6364    0.5313       220\n",
      "         863     0.2088    0.3220    0.2533       118\n",
      "          89     0.9882    0.6980    0.8181       957\n",
      "         908     0.3953    0.7083    0.5075        24\n",
      "         918     0.3699    0.6000    0.4576        90\n",
      "          94     0.2342    0.6096    0.3384       292\n",
      "\n",
      "    accuracy                         0.5275     14202\n",
      "   macro avg     0.4231    0.5157    0.4460     14202\n",
      "weighted avg     0.6314    0.5275    0.5381     14202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test4.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_descr_replaced_subj_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_replaced_subject_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_replaced_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=60, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.7550    0.3542    0.4822      1070\n",
      "         120     0.2665    0.6378    0.3759       196\n",
      "         125     0.7588    0.7688    0.7638       532\n",
      "         134     0.6364    0.7368    0.6829        19\n",
      "         189     0.3152    0.4874    0.3828       119\n",
      "         190     0.4651    0.7000    0.5589       200\n",
      "          20     0.4965    0.1765    0.2605       810\n",
      "         200     0.6638    0.3949    0.4952       590\n",
      "         203     0.3846    0.7407    0.5063        27\n",
      "          22     0.8016    0.7568    0.7786       518\n",
      "         254     0.1379    0.1176    0.1270        34\n",
      "         255     0.2929    0.4328    0.3494        67\n",
      "         264     0.5649    0.5368    0.5505       503\n",
      "         269     0.2647    0.3396    0.2975       106\n",
      "         276     0.1471    0.3125    0.2000        64\n",
      "         284     0.2970    0.2439    0.2679       123\n",
      "         287     0.4266    0.5509    0.4809       285\n",
      "         295     0.5165    0.5802    0.5465        81\n",
      "         306     0.3140    0.2872    0.3000        94\n",
      "         310     0.7792    0.7229    0.7500       249\n",
      "         312     0.1081    0.1905    0.1379        42\n",
      "         319     0.4074    0.4314    0.4190        51\n",
      "         326     0.2778    0.1613    0.2041        31\n",
      "         327     0.3226    0.2857    0.3030        35\n",
      "         345     0.1277    0.2308    0.1644        26\n",
      "         347     0.3871    0.5000    0.4364        24\n",
      "         352     0.4054    0.7991    0.5379       453\n",
      "         362     0.3830    0.4426    0.4106       122\n",
      "         399     0.3738    0.5992    0.4604       262\n",
      "         400     0.2947    0.4420    0.3536       138\n",
      "         401     0.3600    0.3462    0.3529        52\n",
      "         415     0.4576    0.6429    0.5347        42\n",
      "         416     0.5788    0.7231    0.6430       325\n",
      "         426     0.4355    0.6429    0.5192        42\n",
      "         427     0.3529    0.5455    0.4286        44\n",
      "         434     0.5627    0.7629    0.6477       194\n",
      "         476     0.4814    0.7534    0.5874       223\n",
      "         502     0.5208    0.7009    0.5976       107\n",
      "         522     0.2785    0.4889    0.3548        90\n",
      "         532     0.3269    0.7727    0.4595        44\n",
      "          59     0.6641    0.7982    0.7250       109\n",
      "         601     0.3714    0.7324    0.4929        71\n",
      "         611     0.3544    0.7935    0.4899        92\n",
      "         617     0.5938    0.5000    0.5429        38\n",
      "         639     0.1695    0.3571    0.2299        28\n",
      "         668     0.1026    0.0889    0.0952        45\n",
      "         732     0.2637    0.2449    0.2540        98\n",
      "          74     0.1224    0.3380    0.1798        71\n",
      "         755     0.1692    0.3929    0.2366        28\n",
      "          77     0.4337    0.4865    0.4586       148\n",
      "         770     0.2500    0.3898    0.3046        59\n",
      "         772     0.4737    0.5294    0.5000        34\n",
      "          78     0.5134    0.7095    0.5957       296\n",
      "         787     0.6710    0.5225    0.5875       890\n",
      "          79     0.9828    0.4295    0.5978      2256\n",
      "         798     0.7455    0.6357    0.6862       129\n",
      "         835     0.4688    0.7317    0.5714        41\n",
      "         843     0.3500    0.4118    0.3784        34\n",
      "         862     0.4560    0.6364    0.5313       220\n",
      "         863     0.2088    0.3220    0.2533       118\n",
      "          89     0.9882    0.6980    0.8181       957\n",
      "         908     0.3953    0.7083    0.5075        24\n",
      "         918     0.3699    0.6000    0.4576        90\n",
      "          94     0.2342    0.6096    0.3384       292\n",
      "\n",
      "    accuracy                         0.5275     14202\n",
      "   macro avg     0.4231    0.5157    0.4460     14202\n",
      "weighted avg     0.6314    0.5275    0.5381     14202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_descr_replaced_subj_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_replaced_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
