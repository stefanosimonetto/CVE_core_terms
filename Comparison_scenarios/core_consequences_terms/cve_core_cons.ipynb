{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on core terms and consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.9024 - acc\n",
      "Epoch 1 - F1 Score: 0.5708\n",
      "Saved best model\n",
      "[0.5707787828566004]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.9018 - accuracy: 0.4944 - val_loss: 1.4796 - val_accuracy: 0.5909\n",
      "Epoch 2/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.3675 - accu\n",
      "Epoch 2 - F1 Score: 0.6063\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.3673 - accuracy: 0.6166 - val_loss: 1.3482 - val_accuracy: 0.6248\n",
      "Epoch 3/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.2747 - accu\n",
      "Epoch 3 - F1 Score: 0.6189\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967]\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.2746 - accuracy: 0.6396 - val_loss: 1.3133 - val_accuracy: 0.6319\n",
      "Epoch 4/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 1.222\n",
      "Epoch 4 - F1 Score: 0.6250\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.2225 - accuracy: 0.6538 - val_loss: 1.2760 - val_accuracy: 0.6380\n",
      "Epoch 5/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1872 - accu\n",
      "Epoch 5 - F1 Score: 0.6387\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766]\n",
      "2336/2336 [==============================] - 9s 4ms/step - loss: 1.1872 - accuracy: 0.6616 - val_loss: 1.2428 - val_accuracy: 0.6478\n",
      "Epoch 6/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 1.1611 - accu\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 1.1608 - accuracy: 0.6689 - val_loss: 1.2357 - val_accuracy: 0.6483\n",
      "Epoch 7/40\n",
      "260/260 [==============================] - 0s 959us/steposs: 1.1373 - accura\n",
      "Epoch 7 - F1 Score: 0.6411\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287]\n",
      "2336/2336 [==============================] - 8s 3ms/step - loss: 1.1370 - accuracy: 0.6739 - val_loss: 1.2318 - val_accuracy: 0.6528\n",
      "Epoch 8/40\n",
      "260/260 [==============================] - 1s 3ms/step lo\n",
      "2336/2336 [==============================] - 14s 6ms/step - loss: 1.1200 - accuracy: 0.6784 - val_loss: 1.2265 - val_accuracy: 0.6488\n",
      "Epoch 9/40\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "Epoch 9 - F1 Score: 0.6413\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418]\n",
      "2336/2336 [==============================] - 18s 8ms/step - loss: 1.1017 - accuracy: 0.6830 - val_loss: 1.2391 - val_accuracy: 0.6552\n",
      "Epoch 10/40\n",
      "260/260 [==============================] - 1s 3ms/step \n",
      "Epoch 10 - F1 Score: 0.6570\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465]\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0872 - accuracy: 0.6860 - val_loss: 1.2013 - val_accuracy: 0.6659\n",
      "Epoch 11/40\n",
      "260/260 [==============================] - 1s 3ms/step lo\n",
      "Epoch 11 - F1 Score: 0.6598\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855]\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0757 - accuracy: 0.6886 - val_loss: 1.1921 - val_accuracy: 0.6652\n",
      "Epoch 12/40\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "Epoch 12 - F1 Score: 0.6608\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398]\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0619 - accuracy: 0.6916 - val_loss: 1.2002 - val_accuracy: 0.6678\n",
      "Epoch 13/40\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "Epoch 13 - F1 Score: 0.6608\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398, 0.6608448192106087]\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0500 - accuracy: 0.6949 - val_loss: 1.1803 - val_accuracy: 0.6682\n",
      "Epoch 14/40\n",
      "260/260 [==============================] - 1s 3ms/step \n",
      "2336/2336 [==============================] - 16s 7ms/step - loss: 1.0390 - accuracy: 0.6989 - val_loss: 1.1839 - val_accuracy: 0.6663\n",
      "Epoch 15/40\n",
      "260/260 [==============================] - 1s 3ms/step los\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0280 - accuracy: 0.7002 - val_loss: 1.1884 - val_accuracy: 0.6645\n",
      "Epoch 16/40\n",
      "260/260 [==============================] - 1s 3ms/step loss\n",
      "2336/2336 [==============================] - 16s 7ms/step - loss: 1.0173 - accuracy: 0.7015 - val_loss: 1.1937 - val_accuracy: 0.6666\n",
      "Epoch 17/40\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "2336/2336 [==============================] - 17s 7ms/step - loss: 1.0069 - accuracy: 0.7050 - val_loss: 1.1947 - val_accuracy: 0.6688\n",
      "Epoch 18/40\n",
      "260/260 [==============================] - 1s 4ms/step\n",
      "2336/2336 [==============================] - 18s 8ms/step - loss: 0.9982 - accuracy: 0.7067 - val_loss: 1.1849 - val_accuracy: 0.6660\n",
      "Epoch 19/40\n",
      "260/260 [==============================] - 0s 1ms/step loss: 0.9883 - accu\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.9883 - accuracy: 0.7100 - val_loss: 1.1797 - val_accuracy: 0.6688\n",
      "Epoch 20/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.9809 \n",
      "2336/2336 [==============================] - 8s 4ms/step - loss: 0.9799 - accuracy: 0.7117 - val_loss: 1.1945 - val_accuracy: 0.6651\n",
      "Epoch 21/40\n",
      "260/260 [==============================] - 1s 3ms/step\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.9720 - accuracy: 0.7140 - val_loss: 1.2138 - val_accuracy: 0.6614\n",
      "Epoch 22/40\n",
      "260/260 [==============================] - 1s 3ms/step los\n",
      "Epoch 22 - F1 Score: 0.6608\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398, 0.6608448192106087, 0.6594353027942188, 0.6541145627156528, 0.6588181941350731, 0.660490613580583, 0.6548685706087657, 0.6578823941184754, 0.6567737031526534, 0.6563929846416181, 0.6608491252226324]\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.9636 - accuracy: 0.7153 - val_loss: 1.1894 - val_accuracy: 0.6695\n",
      "Epoch 23/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "Epoch 23 - F1 Score: 0.6650\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398, 0.6608448192106087, 0.6594353027942188, 0.6541145627156528, 0.6588181941350731, 0.660490613580583, 0.6548685706087657, 0.6578823941184754, 0.6567737031526534, 0.6563929846416181, 0.6608491252226324, 0.6649812802819083]\n",
      "2336/2336 [==============================] - 10s 4ms/step - loss: 0.9549 - accuracy: 0.7172 - val_loss: 1.1822 - val_accuracy: 0.6704\n",
      "Epoch 24/40\n",
      "260/260 [==============================] - 0s 2ms/step loss: 0.947\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.9462 - accuracy: 0.7192 - val_loss: 1.2012 - val_accuracy: 0.6687\n",
      "Epoch 25/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 14s 6ms/step - loss: 0.9388 - accuracy: 0.7210 - val_loss: 1.1871 - val_accuracy: 0.6707\n",
      "Epoch 26/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.9295 - accuracy: 0.7238 - val_loss: 1.2001 - val_accuracy: 0.6677\n",
      "Epoch 27/40\n",
      "260/260 [==============================] - 1s 2ms/step loss:\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.9238 - accuracy: 0.7234 - val_loss: 1.2384 - val_accuracy: 0.6623\n",
      "Epoch 28/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.913\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.9136 - accuracy: 0.7265 - val_loss: 1.1976 - val_accuracy: 0.6711\n",
      "Epoch 29/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "Epoch 29 - F1 Score: 0.6657\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398, 0.6608448192106087, 0.6594353027942188, 0.6541145627156528, 0.6588181941350731, 0.660490613580583, 0.6548685706087657, 0.6578823941184754, 0.6567737031526534, 0.6563929846416181, 0.6608491252226324, 0.6649812802819083, 0.660639654118623, 0.664766320546884, 0.6613293815152049, 0.6583017733736617, 0.6634598983736384, 0.6657054402615102]\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.9079 - accuracy: 0.7274 - val_loss: 1.2212 - val_accuracy: 0.6706\n",
      "Epoch 30/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.9\n",
      "2336/2336 [==============================] - 13s 5ms/step - loss: 0.9026 - accuracy: 0.7292 - val_loss: 1.2057 - val_accuracy: 0.6690\n",
      "Epoch 31/40\n",
      "260/260 [==============================] - 1s 2ms/step loss:\n",
      "Epoch 31 - F1 Score: 0.6691\n",
      "Saved best model\n",
      "[0.5707787828566004, 0.6063319005153387, 0.6189369599605967, 0.6249525328622582, 0.6386961267347766, 0.6366581967196911, 0.6411171422638287, 0.637564884546139, 0.6412630228910418, 0.6569696590510465, 0.6597920979946855, 0.6607618057445398, 0.6608448192106087, 0.6594353027942188, 0.6541145627156528, 0.6588181941350731, 0.660490613580583, 0.6548685706087657, 0.6578823941184754, 0.6567737031526534, 0.6563929846416181, 0.6608491252226324, 0.6649812802819083, 0.660639654118623, 0.664766320546884, 0.6613293815152049, 0.6583017733736617, 0.6634598983736384, 0.6657054402615102, 0.6595122919403548, 0.6691190468534273]\n",
      "2336/2336 [==============================] - 14s 6ms/step - loss: 0.8935 - accuracy: 0.7321 - val_loss: 1.2030 - val_accuracy: 0.6707\n",
      "Epoch 32/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.8873 - accuracy: 0.7345 - val_loss: 1.2161 - val_accuracy: 0.6687\n",
      "Epoch 33/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.8809 - accuracy: 0.7353 - val_loss: 1.2473 - val_accuracy: 0.6642\n",
      "Epoch 34/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.8749 - accuracy: 0.7369 - val_loss: 1.2635 - val_accuracy: 0.6536\n",
      "Epoch 35/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.866\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.8668 - accuracy: 0.7397 - val_loss: 1.2288 - val_accuracy: 0.6675\n",
      "Epoch 36/40\n",
      "260/260 [==============================] - 1s 3ms/step l\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.8604 - accuracy: 0.7415 - val_loss: 1.2195 - val_accuracy: 0.6699\n",
      "Epoch 37/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2336/2336 [==============================] - 12s 5ms/step - loss: 0.8545 - accuracy: 0.7418 - val_loss: 1.2483 - val_accuracy: 0.6605\n",
      "Epoch 38/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.8493 - accuracy: 0.7435 - val_loss: 1.2747 - val_accuracy: 0.6604\n",
      "Epoch 39/40\n",
      "260/260 [==============================] - 1s 3ms/step loss:\n",
      "2336/2336 [==============================] - 14s 6ms/step - loss: 0.8432 - accuracy: 0.7450 - val_loss: 1.2620 - val_accuracy: 0.6618\n",
      "Epoch 40/40\n",
      "260/260 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2336/2336 [==============================] - 13s 6ms/step - loss: 0.8364 - accuracy: 0.7464 - val_loss: 1.2574 - val_accuracy: 0.6684\n",
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8061    0.4467    0.5749      1070\n",
      "         120     0.3931    0.6378    0.4864       196\n",
      "         125     0.7504    0.8759    0.8083       532\n",
      "         134     0.6429    0.9474    0.7660        19\n",
      "         189     0.6937    0.6471    0.6696       119\n",
      "         190     0.7071    0.8450    0.7699       200\n",
      "          20     0.4440    0.2790    0.3427       810\n",
      "         200     0.5479    0.5136    0.5302       590\n",
      "         203     0.6000    0.7778    0.6774        27\n",
      "          22     0.8619    0.8919    0.8767       518\n",
      "         254     0.0769    0.1176    0.0930        34\n",
      "         255     0.2941    0.2239    0.2542        67\n",
      "         264     0.5051    0.3956    0.4437       503\n",
      "         269     0.2432    0.5094    0.3293       106\n",
      "         276     0.2738    0.3594    0.3108        64\n",
      "         284     0.2368    0.1463    0.1809       123\n",
      "         287     0.4528    0.6561    0.5358       285\n",
      "         295     0.4240    0.6543    0.5146        81\n",
      "         306     0.3415    0.2979    0.3182        94\n",
      "         310     0.7203    0.7550    0.7373       249\n",
      "         312     0.1500    0.1429    0.1463        42\n",
      "         319     0.4677    0.5686    0.5133        51\n",
      "         326     0.4286    0.1935    0.2667        31\n",
      "         327     0.4286    0.4286    0.4286        35\n",
      "         345     0.1304    0.3462    0.1895        26\n",
      "         347     0.4828    0.5833    0.5283        24\n",
      "         352     0.6263    0.9581    0.7574       453\n",
      "         362     0.7007    0.7869    0.7413       122\n",
      "         399     0.4125    0.3779    0.3944       262\n",
      "         400     0.4436    0.4275    0.4354       138\n",
      "         401     0.4394    0.5577    0.4915        52\n",
      "         415     0.7347    0.8571    0.7912        42\n",
      "         416     0.7769    0.8677    0.8198       325\n",
      "         426     0.5217    0.5714    0.5455        42\n",
      "         427     0.5000    0.7500    0.6000        44\n",
      "         434     0.5253    0.8557    0.6510       194\n",
      "         476     0.7895    0.8072    0.7982       223\n",
      "         502     0.7431    0.7570    0.7500       107\n",
      "         522     0.3105    0.7556    0.4401        90\n",
      "         532     0.5745    0.6136    0.5934        44\n",
      "          59     0.7431    0.7431    0.7431       109\n",
      "         601     0.4122    0.8592    0.5571        71\n",
      "         611     0.8058    0.9022    0.8513        92\n",
      "         617     0.5357    0.7895    0.6383        38\n",
      "         639     0.5333    0.5714    0.5517        28\n",
      "         668     0.1404    0.1778    0.1569        45\n",
      "         732     0.2576    0.3469    0.2957        98\n",
      "          74     0.2033    0.3521    0.2577        71\n",
      "         755     0.5000    0.4643    0.4815        28\n",
      "          77     0.5714    0.4054    0.4743       148\n",
      "         770     0.3167    0.3220    0.3193        59\n",
      "         772     0.6667    0.4706    0.5517        34\n",
      "          78     0.5526    0.7635    0.6411       296\n",
      "         787     0.6473    0.7404    0.6908       890\n",
      "          79     0.9968    0.6977    0.8209      2256\n",
      "         798     0.6824    0.7829    0.7292       129\n",
      "         835     0.6304    0.7073    0.6667        41\n",
      "         843     0.6250    0.7353    0.6757        34\n",
      "         862     0.5051    0.6818    0.5803       220\n",
      "         863     0.3375    0.2288    0.2727       118\n",
      "          89     0.9931    0.8986    0.9435       957\n",
      "         908     0.5000    0.5417    0.5200        24\n",
      "         918     0.8736    0.8444    0.8588        90\n",
      "          94     0.5095    0.6438    0.5688       292\n",
      "\n",
      "    accuracy                         0.6465     14202\n",
      "   macro avg     0.5272    0.5915    0.5461     14202\n",
      "weighted avg     0.6821    0.6465    0.6484     14202\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_without_test4.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_core_cons_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_core_consequences_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_core_consequences_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8061    0.4467    0.5749      1070\n",
      "         120     0.3931    0.6378    0.4864       196\n",
      "         125     0.7504    0.8759    0.8083       532\n",
      "         134     0.6429    0.9474    0.7660        19\n",
      "         189     0.6937    0.6471    0.6696       119\n",
      "         190     0.7071    0.8450    0.7699       200\n",
      "          20     0.4440    0.2790    0.3427       810\n",
      "         200     0.5479    0.5136    0.5302       590\n",
      "         203     0.6000    0.7778    0.6774        27\n",
      "          22     0.8619    0.8919    0.8767       518\n",
      "         254     0.0769    0.1176    0.0930        34\n",
      "         255     0.2941    0.2239    0.2542        67\n",
      "         264     0.5051    0.3956    0.4437       503\n",
      "         269     0.2432    0.5094    0.3293       106\n",
      "         276     0.2738    0.3594    0.3108        64\n",
      "         284     0.2368    0.1463    0.1809       123\n",
      "         287     0.4528    0.6561    0.5358       285\n",
      "         295     0.4240    0.6543    0.5146        81\n",
      "         306     0.3415    0.2979    0.3182        94\n",
      "         310     0.7203    0.7550    0.7373       249\n",
      "         312     0.1500    0.1429    0.1463        42\n",
      "         319     0.4677    0.5686    0.5133        51\n",
      "         326     0.4286    0.1935    0.2667        31\n",
      "         327     0.4286    0.4286    0.4286        35\n",
      "         345     0.1304    0.3462    0.1895        26\n",
      "         347     0.4828    0.5833    0.5283        24\n",
      "         352     0.6263    0.9581    0.7574       453\n",
      "         362     0.7007    0.7869    0.7413       122\n",
      "         399     0.4125    0.3779    0.3944       262\n",
      "         400     0.4436    0.4275    0.4354       138\n",
      "         401     0.4394    0.5577    0.4915        52\n",
      "         415     0.7347    0.8571    0.7912        42\n",
      "         416     0.7769    0.8677    0.8198       325\n",
      "         426     0.5217    0.5714    0.5455        42\n",
      "         427     0.5000    0.7500    0.6000        44\n",
      "         434     0.5253    0.8557    0.6510       194\n",
      "         476     0.7895    0.8072    0.7982       223\n",
      "         502     0.7431    0.7570    0.7500       107\n",
      "         522     0.3105    0.7556    0.4401        90\n",
      "         532     0.5745    0.6136    0.5934        44\n",
      "          59     0.7431    0.7431    0.7431       109\n",
      "         601     0.4122    0.8592    0.5571        71\n",
      "         611     0.8058    0.9022    0.8513        92\n",
      "         617     0.5357    0.7895    0.6383        38\n",
      "         639     0.5333    0.5714    0.5517        28\n",
      "         668     0.1404    0.1778    0.1569        45\n",
      "         732     0.2576    0.3469    0.2957        98\n",
      "          74     0.2033    0.3521    0.2577        71\n",
      "         755     0.5000    0.4643    0.4815        28\n",
      "          77     0.5714    0.4054    0.4743       148\n",
      "         770     0.3167    0.3220    0.3193        59\n",
      "         772     0.6667    0.4706    0.5517        34\n",
      "          78     0.5526    0.7635    0.6411       296\n",
      "         787     0.6473    0.7404    0.6908       890\n",
      "          79     0.9968    0.6977    0.8209      2256\n",
      "         798     0.6824    0.7829    0.7292       129\n",
      "         835     0.6304    0.7073    0.6667        41\n",
      "         843     0.6250    0.7353    0.6757        34\n",
      "         862     0.5051    0.6818    0.5803       220\n",
      "         863     0.3375    0.2288    0.2727       118\n",
      "          89     0.9931    0.8986    0.9435       957\n",
      "         908     0.5000    0.5417    0.5200        24\n",
      "         918     0.8736    0.8444    0.8588        90\n",
      "          94     0.5095    0.6438    0.5688       292\n",
      "\n",
      "    accuracy                         0.6465     14202\n",
      "   macro avg     0.5272    0.5915    0.5461     14202\n",
      "weighted avg     0.6821    0.6465    0.6484     14202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('test_core_cons_terms_with_embeddings.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_core_consequences_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
