{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for BERT based on CVE descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "141/141 [==============================] - 0s 955us/steposs: 2.3044 - accuracy: \n",
      "Epoch 1 - F1 Score: 0.4408\n",
      "Saved best model\n",
      "[0.4407841720225724]\n",
      "1266/1266 [==============================] - 4s 2ms/step - loss: 2.2967 - accuracy: 0.4261 - val_loss: 1.9010 - val_accuracy: 0.4916\n",
      "Epoch 2/40\n",
      "141/141 [==============================] - 0s 943us/steposs: 1.7041 - accuracy: \n",
      "Epoch 2 - F1 Score: 0.5075\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.7036 - accuracy: 0.5435 - val_loss: 1.6904 - val_accuracy: 0.5422\n",
      "Epoch 3/40\n",
      "141/141 [==============================] - 0s 905us/steposs: 1.5495 - accuracy: \n",
      "Epoch 3 - F1 Score: 0.5399\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.5474 - accuracy: 0.5788 - val_loss: 1.5831 - val_accuracy: 0.5671\n",
      "Epoch 4/40\n",
      "141/141 [==============================] - 0s 948us/steposs: 1.4551 - accuracy: \n",
      "Epoch 4 - F1 Score: 0.5640\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.4548 - accuracy: 0.6010 - val_loss: 1.5176 - val_accuracy: 0.5927\n",
      "Epoch 5/40\n",
      "141/141 [==============================] - 0s 877us/steposs: 1.3894 - accuracy: \n",
      "Epoch 5 - F1 Score: 0.5780\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.3892 - accuracy: 0.6149 - val_loss: 1.4848 - val_accuracy: 0.6011\n",
      "Epoch 6/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.3347 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.3363 - accuracy: 0.6280 - val_loss: 1.5119 - val_accuracy: 0.5864\n",
      "Epoch 7/40\n",
      "141/141 [==============================] - 0s 888us/steposs: 1.2992 - accuracy: \n",
      "Epoch 7 - F1 Score: 0.5829\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987]\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.2996 - accuracy: 0.6359 - val_loss: 1.4567 - val_accuracy: 0.6078\n",
      "Epoch 8/40\n",
      "141/141 [==============================] - 0s 890us/steposs: 1.2573 - accuracy: \n",
      "Epoch 8 - F1 Score: 0.5914\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.2578 - accuracy: 0.6465 - val_loss: 1.4197 - val_accuracy: 0.6116\n",
      "Epoch 9/40\n",
      "141/141 [==============================] - 0s 917us/steposs: 1.2285 - accuracy: \n",
      "Epoch 9 - F1 Score: 0.5985\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.2286 - accuracy: 0.6556 - val_loss: 1.4386 - val_accuracy: 0.6102\n",
      "Epoch 10/40\n",
      "141/141 [==============================] - 0s 880us/steposs: 1.2024 - accuracy: \n",
      "1266/1266 [==============================] - 2s 2ms/step - loss: 1.2023 - accuracy: 0.6592 - val_loss: 1.4026 - val_accuracy: 0.6171\n",
      "Epoch 11/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.1736 - ac\n",
      "Epoch 11 - F1 Score: 0.6136\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563, 0.5969021074753988, 0.6135587212485359]\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 1.1735 - accuracy: 0.6643 - val_loss: 1.3898 - val_accuracy: 0.6256\n",
      "Epoch 12/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.1493 - accura\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.1495 - accuracy: 0.6722 - val_loss: 1.3796 - val_accuracy: 0.6280\n",
      "Epoch 13/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1311 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1310 - accuracy: 0.6759 - val_loss: 1.4228 - val_accuracy: 0.6227\n",
      "Epoch 14/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1140 - accuracy\n",
      "Epoch 14 - F1 Score: 0.6168\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563, 0.5969021074753988, 0.6135587212485359, 0.6078307921962571, 0.6064681817754914, 0.6168400381157173]\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.1153 - accuracy: 0.6797 - val_loss: 1.3955 - val_accuracy: 0.6233\n",
      "Epoch 15/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0934 - accuracy\n",
      "Epoch 15 - F1 Score: 0.6207\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563, 0.5969021074753988, 0.6135587212485359, 0.6078307921962571, 0.6064681817754914, 0.6168400381157173, 0.6206603439760091]\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 1.0925 - accuracy: 0.6874 - val_loss: 1.4017 - val_accuracy: 0.6358\n",
      "Epoch 16/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.0780 - accura\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.0781 - accuracy: 0.6899 - val_loss: 1.4311 - val_accuracy: 0.6156\n",
      "Epoch 17/40\n",
      "141/141 [==============================] - 0s 956us/steposs: 1.0569 - accuracy: \n",
      "1266/1266 [==============================] - 4s 4ms/step - loss: 1.0574 - accuracy: 0.6947 - val_loss: 1.3983 - val_accuracy: 0.6289\n",
      "Epoch 18/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0482 - accuracy: \n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.0480 - accuracy: 0.6960 - val_loss: 1.4579 - val_accuracy: 0.6209\n",
      "Epoch 19/40\n",
      "141/141 [==============================] - 0s 918us/steposs: 1.0343 - accuracy: \n",
      "1266/1266 [==============================] - 4s 4ms/step - loss: 1.0348 - accuracy: 0.6986 - val_loss: 1.3906 - val_accuracy: 0.6273\n",
      "Epoch 20/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0180 - accuracy\n",
      "1266/1266 [==============================] - 6s 5ms/step - loss: 1.0181 - accuracy: 0.7045 - val_loss: 1.4537 - val_accuracy: 0.6218\n",
      "Epoch 21/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0062 - accuracy\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.0055 - accuracy: 0.7059 - val_loss: 1.4349 - val_accuracy: 0.6271\n",
      "Epoch 22/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.9944 - accu\n",
      "1266/1266 [==============================] - 6s 4ms/step - loss: 0.9939 - accuracy: 0.7091 - val_loss: 1.4254 - val_accuracy: 0.6289\n",
      "Epoch 23/40\n",
      "141/141 [==============================] - 0s 877us/steposs: 0.9797 - accuracy: \n",
      "1266/1266 [==============================] - 5s 4ms/step - loss: 0.9799 - accuracy: 0.7121 - val_loss: 1.4587 - val_accuracy: 0.6204\n",
      "Epoch 24/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9731 - accuracy\n",
      "Epoch 24 - F1 Score: 0.6324\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563, 0.5969021074753988, 0.6135587212485359, 0.6078307921962571, 0.6064681817754914, 0.6168400381157173, 0.6206603439760091, 0.6050053424396779, 0.6161148955597288, 0.6003086935760309, 0.6118229201874673, 0.6097591648924837, 0.6100482469852323, 0.6136176945242582, 0.613886442278108, 0.6323729639054949]\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.9740 - accuracy: 0.7149 - val_loss: 1.3686 - val_accuracy: 0.6411\n",
      "Epoch 25/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.9605 - ac\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9599 - accuracy: 0.7177 - val_loss: 1.4056 - val_accuracy: 0.6340\n",
      "Epoch 26/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9513 - accuracy: \n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.9513 - accuracy: 0.7175 - val_loss: 1.4058 - val_accuracy: 0.6296\n",
      "Epoch 27/40\n",
      "141/141 [==============================] - 0s 855us/steposs: 0.9370 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9370 - accuracy: 0.7220 - val_loss: 1.4078 - val_accuracy: 0.6338\n",
      "Epoch 28/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.9249 - accu\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.9250 - accuracy: 0.7260 - val_loss: 1.4281 - val_accuracy: 0.6311\n",
      "Epoch 29/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9120 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9122 - accuracy: 0.7284 - val_loss: 1.4747 - val_accuracy: 0.6289\n",
      "Epoch 30/40\n",
      "141/141 [==============================] - 0s 925us/steposs: 0.9098 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9100 - accuracy: 0.7285 - val_loss: 1.4323 - val_accuracy: 0.6282\n",
      "Epoch 31/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8943 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8944 - accuracy: 0.7337 - val_loss: 1.4652 - val_accuracy: 0.6364\n",
      "Epoch 32/40\n",
      "141/141 [==============================] - 0s 929us/steposs: 0.8890 - accuracy: \n",
      "1266/1266 [==============================] - 2s 2ms/step - loss: 0.8891 - accuracy: 0.7346 - val_loss: 1.4410 - val_accuracy: 0.6318\n",
      "Epoch 33/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8779 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8792 - accuracy: 0.7369 - val_loss: 1.4309 - val_accuracy: 0.6344\n",
      "Epoch 34/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8715 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8717 - accuracy: 0.7400 - val_loss: 1.4517 - val_accuracy: 0.6362\n",
      "Epoch 35/40\n",
      "141/141 [==============================] - 0s 981us/steposs: 0.8657 - accuracy: \n",
      "1266/1266 [==============================] - 2s 2ms/step - loss: 0.8665 - accuracy: 0.7381 - val_loss: 1.4488 - val_accuracy: 0.6273\n",
      "Epoch 36/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8573 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8578 - accuracy: 0.7400 - val_loss: 1.4587 - val_accuracy: 0.6316\n",
      "Epoch 37/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.8421 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8421 - accuracy: 0.7459 - val_loss: 1.4815 - val_accuracy: 0.6280\n",
      "Epoch 38/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8384 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8398 - accuracy: 0.7444 - val_loss: 1.4717 - val_accuracy: 0.6327\n",
      "Epoch 39/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8285 - accuracy\n",
      "Epoch 39 - F1 Score: 0.6333\n",
      "Saved best model\n",
      "[0.4407841720225724, 0.5074592719286187, 0.5399428219271998, 0.5639548241222992, 0.5780211731920412, 0.5667529985536977, 0.5828855072510987, 0.5913998502417958, 0.5985009604915563, 0.5969021074753988, 0.6135587212485359, 0.6078307921962571, 0.6064681817754914, 0.6168400381157173, 0.6206603439760091, 0.6050053424396779, 0.6161148955597288, 0.6003086935760309, 0.6118229201874673, 0.6097591648924837, 0.6100482469852323, 0.6136176945242582, 0.613886442278108, 0.6323729639054949, 0.6227205911720624, 0.6231250632284538, 0.6284408751305657, 0.6219752059161522, 0.6147015443929553, 0.6204300617770913, 0.6216893377897629, 0.6197393626186372, 0.6241847651356324, 0.6271964803926193, 0.6226404336323037, 0.6219726051147761, 0.6207401672150096, 0.6235563977363547, 0.6333478143069803]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8290 - accuracy: 0.7489 - val_loss: 1.4958 - val_accuracy: 0.6387\n",
      "Epoch 40/40\n",
      "141/141 [==============================] - 0s 931us/steposs: 0.8218 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8215 - accuracy: 0.7525 - val_loss: 1.5166 - val_accuracy: 0.6333\n",
      "444/444 [==============================] - 0s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.6478    0.5019    0.5656      1070\n",
      "         120     0.4245    0.4592    0.4412       196\n",
      "         125     0.0000    0.0000    0.0000       532\n",
      "         134     0.2500    0.6316    0.3582        19\n",
      "         189     0.6364    0.1765    0.2763       119\n",
      "         190     0.6576    0.6050    0.6302       200\n",
      "          20     0.3430    0.3519    0.3473       810\n",
      "         200     0.5386    0.4492    0.4898       590\n",
      "         203     0.5625    0.3333    0.4186        27\n",
      "          22     0.4714    0.9228    0.6240       518\n",
      "         254     0.0476    0.0882    0.0619        34\n",
      "         255     0.2434    0.5522    0.3379        67\n",
      "         264     0.5409    0.5785    0.5591       503\n",
      "         269     0.2424    0.0755    0.1151       106\n",
      "         276     0.1500    0.4688    0.2273        64\n",
      "         284     0.3913    0.0732    0.1233       123\n",
      "         287     0.3730    0.4947    0.4253       285\n",
      "         295     0.8235    0.1728    0.2857        81\n",
      "         306     0.5294    0.0957    0.1622        94\n",
      "         310     0.9135    0.6787    0.7788       249\n",
      "         312     0.1364    0.2857    0.1846        42\n",
      "         319     0.1545    0.6667    0.2509        51\n",
      "         326     0.1304    0.0968    0.1111        31\n",
      "         327     0.2090    0.4000    0.2745        35\n",
      "         345     0.1957    0.3462    0.2500        26\n",
      "         347     0.3889    0.5833    0.4667        24\n",
      "         352     0.5080    0.8366    0.6322       453\n",
      "         362     0.6351    0.3852    0.4796       122\n",
      "         399     0.4010    0.3015    0.3442       262\n",
      "         400     0.2286    0.0580    0.0925       138\n",
      "         401     0.1638    0.5577    0.2533        52\n",
      "         415     0.1324    0.6905    0.2222        42\n",
      "         416     0.0000    0.0000    0.0000       325\n",
      "         426     0.3607    0.5238    0.4272        42\n",
      "         427     0.1510    0.8409    0.2561        44\n",
      "         434     0.6991    0.4072    0.5147       194\n",
      "         476     0.6854    0.6547    0.6697       223\n",
      "         502     0.7857    0.2056    0.3259       107\n",
      "         522     0.4000    0.1333    0.2000        90\n",
      "         532     0.3656    0.7727    0.4964        44\n",
      "          59     0.8947    0.3119    0.4626       109\n",
      "         601     0.2197    0.8169    0.3463        71\n",
      "         611     0.6552    0.6196    0.6369        92\n",
      "         617     0.2874    0.6579    0.4000        38\n",
      "         639     0.1200    0.2143    0.1538        28\n",
      "         668     0.0678    0.1778    0.0982        45\n",
      "         732     0.2308    0.0306    0.0541        98\n",
      "          74     0.0549    0.3521    0.0951        71\n",
      "         755     0.0821    0.3929    0.1358        28\n",
      "          77     0.6875    0.0743    0.1341       148\n",
      "         770     0.1930    0.3729    0.2543        59\n",
      "         772     0.2400    0.5294    0.3303        34\n",
      "          78     0.4698    0.7365    0.5737       296\n",
      "         787     0.4024    0.7506    0.5239       890\n",
      "          79     0.9712    0.5230    0.6799      2256\n",
      "         798     0.6122    0.2326    0.3371       129\n",
      "         835     0.3253    0.6585    0.4355        41\n",
      "         843     0.5208    0.7353    0.6098        34\n",
      "         862     0.7117    0.3591    0.4773       220\n",
      "         863     0.2000    0.0254    0.0451       118\n",
      "          89     0.9891    0.7618    0.8607       957\n",
      "         908     0.2281    0.5417    0.3210        24\n",
      "         918     0.4198    0.3778    0.3977        90\n",
      "          94     0.4314    0.3767    0.4022       292\n",
      "\n",
      "    accuracy                         0.4881     14202\n",
      "   macro avg     0.3990    0.4232    0.3507     14202\n",
      "weighted avg     0.5666    0.4881    0.4862     14202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train_descr.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('bert_comparison_train_0.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('bert_comparison_test_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_bert_mean'].tolist() for item in balanced if item['cwe'] != 'None'])\n",
    "\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_bert_mean'].tolist() for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model_descr.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train_descr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for CVE description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.6478    0.5019    0.5656      1070\n",
      "         120     0.4245    0.4592    0.4412       196\n",
      "         125     0.0000    0.0000    0.0000       532\n",
      "         134     0.2500    0.6316    0.3582        19\n",
      "         189     0.6364    0.1765    0.2763       119\n",
      "         190     0.6576    0.6050    0.6302       200\n",
      "          20     0.3430    0.3519    0.3473       810\n",
      "         200     0.5386    0.4492    0.4898       590\n",
      "         203     0.5625    0.3333    0.4186        27\n",
      "          22     0.4714    0.9228    0.6240       518\n",
      "         254     0.0476    0.0882    0.0619        34\n",
      "         255     0.2434    0.5522    0.3379        67\n",
      "         264     0.5409    0.5785    0.5591       503\n",
      "         269     0.2424    0.0755    0.1151       106\n",
      "         276     0.1500    0.4688    0.2273        64\n",
      "         284     0.3913    0.0732    0.1233       123\n",
      "         287     0.3730    0.4947    0.4253       285\n",
      "         295     0.8235    0.1728    0.2857        81\n",
      "         306     0.5294    0.0957    0.1622        94\n",
      "         310     0.9135    0.6787    0.7788       249\n",
      "         312     0.1364    0.2857    0.1846        42\n",
      "         319     0.1545    0.6667    0.2509        51\n",
      "         326     0.1304    0.0968    0.1111        31\n",
      "         327     0.2090    0.4000    0.2745        35\n",
      "         345     0.1957    0.3462    0.2500        26\n",
      "         347     0.3889    0.5833    0.4667        24\n",
      "         352     0.5080    0.8366    0.6322       453\n",
      "         362     0.6351    0.3852    0.4796       122\n",
      "         399     0.4010    0.3015    0.3442       262\n",
      "         400     0.2286    0.0580    0.0925       138\n",
      "         401     0.1638    0.5577    0.2533        52\n",
      "         415     0.1324    0.6905    0.2222        42\n",
      "         416     0.0000    0.0000    0.0000       325\n",
      "         426     0.3607    0.5238    0.4272        42\n",
      "         427     0.1510    0.8409    0.2561        44\n",
      "         434     0.6991    0.4072    0.5147       194\n",
      "         476     0.6854    0.6547    0.6697       223\n",
      "         502     0.7857    0.2056    0.3259       107\n",
      "         522     0.4000    0.1333    0.2000        90\n",
      "         532     0.3656    0.7727    0.4964        44\n",
      "          59     0.8947    0.3119    0.4626       109\n",
      "         601     0.2197    0.8169    0.3463        71\n",
      "         611     0.6552    0.6196    0.6369        92\n",
      "         617     0.2874    0.6579    0.4000        38\n",
      "         639     0.1200    0.2143    0.1538        28\n",
      "         668     0.0678    0.1778    0.0982        45\n",
      "         732     0.2308    0.0306    0.0541        98\n",
      "          74     0.0549    0.3521    0.0951        71\n",
      "         755     0.0821    0.3929    0.1358        28\n",
      "          77     0.6875    0.0743    0.1341       148\n",
      "         770     0.1930    0.3729    0.2543        59\n",
      "         772     0.2400    0.5294    0.3303        34\n",
      "          78     0.4698    0.7365    0.5737       296\n",
      "         787     0.4024    0.7506    0.5239       890\n",
      "          79     0.9712    0.5230    0.6799      2256\n",
      "         798     0.6122    0.2326    0.3371       129\n",
      "         835     0.3253    0.6585    0.4355        41\n",
      "         843     0.5208    0.7353    0.6098        34\n",
      "         862     0.7117    0.3591    0.4773       220\n",
      "         863     0.2000    0.0254    0.0451       118\n",
      "          89     0.9891    0.7618    0.8607       957\n",
      "         908     0.2281    0.5417    0.3210        24\n",
      "         918     0.4198    0.3778    0.3977        90\n",
      "          94     0.4314    0.3767    0.4022       292\n",
      "\n",
      "    accuracy                         0.4881     14202\n",
      "   macro avg     0.3990    0.4232    0.3507     14202\n",
      "weighted avg     0.5666    0.4881    0.4862     14202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('bert_comparison_test_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_bert_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model_descr.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train_descr.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for BERT based on CVE core terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.9155 - accura\n",
      "Epoch 1 - F1 Score: 0.5597\n",
      "Saved best model\n",
      "[0.5596929494004437]\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 1.9139 - accuracy: 0.5148 - val_loss: 1.5901 - val_accuracy: 0.5838\n",
      "Epoch 2/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.4485 - accuracy\n",
      "Epoch 2 - F1 Score: 0.5834\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.4471 - accuracy: 0.6142 - val_loss: 1.4779 - val_accuracy: 0.6058\n",
      "Epoch 3/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.3490 - accuracy\n",
      "Epoch 3 - F1 Score: 0.6078\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.3479 - accuracy: 0.6368 - val_loss: 1.4120 - val_accuracy: 0.6296\n",
      "Epoch 4/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.2950 - accuracy\n",
      "Epoch 4 - F1 Score: 0.6130\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.2951 - accuracy: 0.6475 - val_loss: 1.3890 - val_accuracy: 0.6369\n",
      "Epoch 5/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.2560 - accuracy\n",
      "Epoch 5 - F1 Score: 0.6190\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545, 0.6189541120533575]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.2585 - accuracy: 0.6542 - val_loss: 1.3746 - val_accuracy: 0.6380\n",
      "Epoch 6/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.2230 - accuracy\n",
      "Epoch 6 - F1 Score: 0.6283\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545, 0.6189541120533575, 0.6282516929060225]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.2232 - accuracy: 0.6616 - val_loss: 1.3470 - val_accuracy: 0.6449\n",
      "Epoch 7/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1906 - accuracy\n",
      "Epoch 7 - F1 Score: 0.6308\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545, 0.6189541120533575, 0.6282516929060225, 0.6307582904229094]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1929 - accuracy: 0.6680 - val_loss: 1.3378 - val_accuracy: 0.6460\n",
      "Epoch 8/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1669 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1668 - accuracy: 0.6740 - val_loss: 1.3472 - val_accuracy: 0.6398\n",
      "Epoch 9/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1446 - accuracy\n",
      "Epoch 9 - F1 Score: 0.6360\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545, 0.6189541120533575, 0.6282516929060225, 0.6307582904229094, 0.6239148466503224, 0.6359515953010709]\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1451 - accuracy: 0.6768 - val_loss: 1.3404 - val_accuracy: 0.6478\n",
      "Epoch 10/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.1246 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1245 - accuracy: 0.6823 - val_loss: 1.3610 - val_accuracy: 0.6424\n",
      "Epoch 11/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.1071 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.1067 - accuracy: 0.6849 - val_loss: 1.3383 - val_accuracy: 0.6462\n",
      "Epoch 12/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0824 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.0824 - accuracy: 0.6932 - val_loss: 1.3975 - val_accuracy: 0.6329\n",
      "Epoch 13/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0644 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.0645 - accuracy: 0.6952 - val_loss: 1.3404 - val_accuracy: 0.6444\n",
      "Epoch 14/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 1.0498 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.0497 - accuracy: 0.6981 - val_loss: 1.3716 - val_accuracy: 0.6402\n",
      "Epoch 15/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.0332 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.0351 - accuracy: 0.7015 - val_loss: 1.3693 - val_accuracy: 0.6384\n",
      "Epoch 16/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.0189 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 1.0200 - accuracy: 0.7042 - val_loss: 1.3722 - val_accuracy: 0.6402\n",
      "Epoch 17/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 1.0066 - accura\n",
      "Epoch 17 - F1 Score: 0.6423\n",
      "Saved best model\n",
      "[0.5596929494004437, 0.5833781887317446, 0.6078257810541927, 0.6130070526836545, 0.6189541120533575, 0.6282516929060225, 0.6307582904229094, 0.6239148466503224, 0.6359515953010709, 0.6261313577716753, 0.6341979860198588, 0.6159532587509213, 0.6245674222231927, 0.6245728559736631, 0.6241251279039063, 0.6272441264790755, 0.6422870149274931]\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 1.0072 - accuracy: 0.7084 - val_loss: 1.3411 - val_accuracy: 0.6547\n",
      "Epoch 18/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9903 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9904 - accuracy: 0.7118 - val_loss: 1.3988 - val_accuracy: 0.6462\n",
      "Epoch 19/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9735 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9735 - accuracy: 0.7152 - val_loss: 1.4009 - val_accuracy: 0.6502\n",
      "Epoch 20/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9623 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9631 - accuracy: 0.7183 - val_loss: 1.4063 - val_accuracy: 0.6462\n",
      "Epoch 21/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9496 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9492 - accuracy: 0.7203 - val_loss: 1.4298 - val_accuracy: 0.6369\n",
      "Epoch 22/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9329 - accuracy\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.9349 - accuracy: 0.7239 - val_loss: 1.3619 - val_accuracy: 0.6533\n",
      "Epoch 23/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9230 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9239 - accuracy: 0.7259 - val_loss: 1.4158 - val_accuracy: 0.6413\n",
      "Epoch 24/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9064 - accuracy: \n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.9077 - accuracy: 0.7321 - val_loss: 1.4073 - val_accuracy: 0.6484\n",
      "Epoch 25/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.9015 - accuracy\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.9010 - accuracy: 0.7302 - val_loss: 1.4029 - val_accuracy: 0.6460\n",
      "Epoch 26/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8882 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8879 - accuracy: 0.7355 - val_loss: 1.4405 - val_accuracy: 0.6396\n",
      "Epoch 27/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.8781 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8764 - accuracy: 0.7391 - val_loss: 1.4825 - val_accuracy: 0.6476\n",
      "Epoch 28/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8629 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8629 - accuracy: 0.7400 - val_loss: 1.4537 - val_accuracy: 0.6458\n",
      "Epoch 29/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.8528 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8528 - accuracy: 0.7422 - val_loss: 1.4872 - val_accuracy: 0.6391\n",
      "Epoch 30/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.8405 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8427 - accuracy: 0.7454 - val_loss: 1.4366 - val_accuracy: 0.6442\n",
      "Epoch 31/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8274 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8272 - accuracy: 0.7488 - val_loss: 1.5581 - val_accuracy: 0.6369\n",
      "Epoch 32/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.8222 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8227 - accuracy: 0.7492 - val_loss: 1.5122 - val_accuracy: 0.6356\n",
      "Epoch 33/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.8135 - accura\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.8131 - accuracy: 0.7529 - val_loss: 1.5517 - val_accuracy: 0.6313\n",
      "Epoch 34/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7974 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7977 - accuracy: 0.7565 - val_loss: 1.5338 - val_accuracy: 0.6380\n",
      "Epoch 35/40\n",
      "141/141 [==============================] - 0s 2ms/step loss: 0.7930 - accura\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.7933 - accuracy: 0.7568 - val_loss: 1.5805 - val_accuracy: 0.6311\n",
      "Epoch 36/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7826 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7820 - accuracy: 0.7590 - val_loss: 1.5503 - val_accuracy: 0.6344\n",
      "Epoch 37/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7700 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7702 - accuracy: 0.7619 - val_loss: 1.5848 - val_accuracy: 0.6336\n",
      "Epoch 38/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7613 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7611 - accuracy: 0.7649 - val_loss: 1.6102 - val_accuracy: 0.6338\n",
      "Epoch 39/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7539 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7539 - accuracy: 0.7673 - val_loss: 1.5691 - val_accuracy: 0.6318\n",
      "Epoch 40/40\n",
      "141/141 [==============================] - 0s 1ms/step loss: 0.7438 - accuracy\n",
      "1266/1266 [==============================] - 3s 2ms/step - loss: 0.7435 - accuracy: 0.7681 - val_loss: 1.6538 - val_accuracy: 0.6353\n",
      "444/444 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.6253    0.5411    0.5802      1070\n",
      "         120     0.3598    0.3010    0.3278       196\n",
      "         125     0.0000    0.0000    0.0000       532\n",
      "         134     0.4000    0.7368    0.5185        19\n",
      "         189     0.5000    0.1429    0.2222       119\n",
      "         190     0.5335    0.8350    0.6511       200\n",
      "          20     0.3557    0.2679    0.3056       810\n",
      "         200     0.4402    0.5051    0.4704       590\n",
      "         203     0.1939    0.7037    0.3040        27\n",
      "          22     0.7524    0.8977    0.8187       518\n",
      "         254     0.0000    0.0000    0.0000        34\n",
      "         255     0.1429    0.4478    0.2166        67\n",
      "         264     0.3842    0.3201    0.3492       503\n",
      "         269     0.3214    0.1698    0.2222       106\n",
      "         276     0.1141    0.5938    0.1914        64\n",
      "         284     0.6154    0.0650    0.1176       123\n",
      "         287     0.3807    0.5544    0.4514       285\n",
      "         295     0.4186    0.2222    0.2903        81\n",
      "         306     0.1714    0.0638    0.0930        94\n",
      "         310     0.8250    0.6627    0.7350       249\n",
      "         312     0.1724    0.3571    0.2326        42\n",
      "         319     0.3488    0.5882    0.4380        51\n",
      "         326     0.2308    0.1935    0.2105        31\n",
      "         327     0.2727    0.3429    0.3038        35\n",
      "         345     0.1228    0.2692    0.1687        26\n",
      "         347     0.2895    0.4583    0.3548        24\n",
      "         352     0.8812    0.9007    0.8908       453\n",
      "         362     0.7683    0.5164    0.6176       122\n",
      "         399     0.2833    0.3168    0.2991       262\n",
      "         400     0.3500    0.1522    0.2121       138\n",
      "         401     0.2333    0.5385    0.3256        52\n",
      "         415     0.4186    0.8571    0.5625        42\n",
      "         416     0.0000    0.0000    0.0000       325\n",
      "         426     0.5854    0.5714    0.5783        42\n",
      "         427     0.3556    0.7273    0.4776        44\n",
      "         434     0.8014    0.5825    0.6746       194\n",
      "         476     0.7397    0.8027    0.7699       223\n",
      "         502     0.7075    0.7009    0.7042       107\n",
      "         522     0.3571    0.0556    0.0962        90\n",
      "         532     0.3443    0.4773    0.4000        44\n",
      "          59     0.6869    0.6239    0.6538       109\n",
      "         601     0.4331    0.7746    0.5556        71\n",
      "         611     0.8023    0.7500    0.7753        92\n",
      "         617     0.2889    0.6842    0.4062        38\n",
      "         639     0.3953    0.6071    0.4789        28\n",
      "         668     0.0364    0.1333    0.0571        45\n",
      "         732     0.0833    0.0102    0.0182        98\n",
      "          74     0.1080    0.3239    0.1620        71\n",
      "         755     0.1101    0.4286    0.1752        28\n",
      "          77     0.5897    0.1554    0.2460       148\n",
      "         770     0.1310    0.3220    0.1863        59\n",
      "         772     0.2135    0.5588    0.3089        34\n",
      "          78     0.4457    0.8041    0.5735       296\n",
      "         787     0.4859    0.7371    0.5857       890\n",
      "          79     0.9890    0.8338    0.9048      2256\n",
      "         798     0.6364    0.4341    0.5161       129\n",
      "         835     0.3111    0.6829    0.4275        41\n",
      "         843     0.5909    0.7647    0.6667        34\n",
      "         862     0.5303    0.4773    0.5024       220\n",
      "         863     0.1053    0.0169    0.0292       118\n",
      "          89     0.9815    0.8882    0.9325       957\n",
      "         908     0.2941    0.6250    0.4000        24\n",
      "         918     0.9143    0.7111    0.8000        90\n",
      "          94     0.3680    0.5205    0.4312       292\n",
      "\n",
      "    accuracy                         0.5645     14202\n",
      "   macro avg     0.4114    0.4736    0.4059     14202\n",
      "weighted avg     0.5788    0.5645    0.5545     14202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train_terms.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('bert_comparison_train_0.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('bert_comparison_test_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_terms_bert_mean'].tolist() for item in balanced if item['cwe'] != 'None'])\n",
    "\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_terms_bert_mean'].tolist() for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model_terms.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train_terms.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for core terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.6253    0.5411    0.5802      1070\n",
      "         120     0.3598    0.3010    0.3278       196\n",
      "         125     0.0000    0.0000    0.0000       532\n",
      "         134     0.4000    0.7368    0.5185        19\n",
      "         189     0.5000    0.1429    0.2222       119\n",
      "         190     0.5335    0.8350    0.6511       200\n",
      "          20     0.3557    0.2679    0.3056       810\n",
      "         200     0.4402    0.5051    0.4704       590\n",
      "         203     0.1939    0.7037    0.3040        27\n",
      "          22     0.7524    0.8977    0.8187       518\n",
      "         254     0.0000    0.0000    0.0000        34\n",
      "         255     0.1429    0.4478    0.2166        67\n",
      "         264     0.3842    0.3201    0.3492       503\n",
      "         269     0.3214    0.1698    0.2222       106\n",
      "         276     0.1141    0.5938    0.1914        64\n",
      "         284     0.6154    0.0650    0.1176       123\n",
      "         287     0.3807    0.5544    0.4514       285\n",
      "         295     0.4186    0.2222    0.2903        81\n",
      "         306     0.1714    0.0638    0.0930        94\n",
      "         310     0.8250    0.6627    0.7350       249\n",
      "         312     0.1724    0.3571    0.2326        42\n",
      "         319     0.3488    0.5882    0.4380        51\n",
      "         326     0.2308    0.1935    0.2105        31\n",
      "         327     0.2727    0.3429    0.3038        35\n",
      "         345     0.1228    0.2692    0.1687        26\n",
      "         347     0.2895    0.4583    0.3548        24\n",
      "         352     0.8812    0.9007    0.8908       453\n",
      "         362     0.7683    0.5164    0.6176       122\n",
      "         399     0.2833    0.3168    0.2991       262\n",
      "         400     0.3500    0.1522    0.2121       138\n",
      "         401     0.2333    0.5385    0.3256        52\n",
      "         415     0.4186    0.8571    0.5625        42\n",
      "         416     0.0000    0.0000    0.0000       325\n",
      "         426     0.5854    0.5714    0.5783        42\n",
      "         427     0.3556    0.7273    0.4776        44\n",
      "         434     0.8014    0.5825    0.6746       194\n",
      "         476     0.7397    0.8027    0.7699       223\n",
      "         502     0.7075    0.7009    0.7042       107\n",
      "         522     0.3571    0.0556    0.0962        90\n",
      "         532     0.3443    0.4773    0.4000        44\n",
      "          59     0.6869    0.6239    0.6538       109\n",
      "         601     0.4331    0.7746    0.5556        71\n",
      "         611     0.8023    0.7500    0.7753        92\n",
      "         617     0.2889    0.6842    0.4062        38\n",
      "         639     0.3953    0.6071    0.4789        28\n",
      "         668     0.0364    0.1333    0.0571        45\n",
      "         732     0.0833    0.0102    0.0182        98\n",
      "          74     0.1080    0.3239    0.1620        71\n",
      "         755     0.1101    0.4286    0.1752        28\n",
      "          77     0.5897    0.1554    0.2460       148\n",
      "         770     0.1310    0.3220    0.1863        59\n",
      "         772     0.2135    0.5588    0.3089        34\n",
      "          78     0.4457    0.8041    0.5735       296\n",
      "         787     0.4859    0.7371    0.5857       890\n",
      "          79     0.9890    0.8338    0.9048      2256\n",
      "         798     0.6364    0.4341    0.5161       129\n",
      "         835     0.3111    0.6829    0.4275        41\n",
      "         843     0.5909    0.7647    0.6667        34\n",
      "         862     0.5303    0.4773    0.5024       220\n",
      "         863     0.1053    0.0169    0.0292       118\n",
      "          89     0.9815    0.8882    0.9325       957\n",
      "         908     0.2941    0.6250    0.4000        24\n",
      "         918     0.9143    0.7111    0.8000        90\n",
      "          94     0.3680    0.5205    0.4312       292\n",
      "\n",
      "    accuracy                         0.5645     14202\n",
      "   macro avg     0.4114    0.4736    0.4059     14202\n",
      "weighted avg     0.5788    0.5645    0.5545     14202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Save the best model\n",
    "with open('bert_comparison_test_0.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_terms_bert_mean'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "best_model=joblib.load('best_model_terms.joblib')\n",
    "label_encoder_train=joblib.load('label_encoder_train_terms.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
